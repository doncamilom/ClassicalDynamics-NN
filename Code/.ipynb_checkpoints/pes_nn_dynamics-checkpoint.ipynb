{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "- [x] Build data set.\n",
    "- [x] Build NN infrastructure:\n",
    "   - [x] How to compute gradients\n",
    "   - [x] Select architecture\n",
    "   - [x] Tune hyperparameters\n",
    "- [x] Train and test NN.\n",
    "- [x] Run some dynamics on the new PES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build data set.\n",
    "Our system is H$_2$CO$^+$, a charged, open shell molecular system. The idea is to start from chemically relevant structures such as some minima and transition states, such as those reported at [Moyano 2006](https://aip.scitation.org/doi/10.1063/1.2181571), as shown in Figure 1. I will for simplicity not include all of these structure, but only the most representative of the PES (the lowest energy configurations: ).\n",
    "\n",
    "<img src=\"./Structs.png\" width='700'>\n",
    "\n",
    "The idea for now is to start from these structures and randomly sample other structures around these, so non-important regions of the PES are not explored in detriment of the important ones. (Though there are much better approximations to sampling, we'll be doing this for now, for simplicity).\n",
    "\n",
    "As the inputs we'll be using the inverses of distances between ALL pairs of atoms, so as to fully describe the molecule's geometry and account for shorter distances representing stronger interactions.\n",
    "\n",
    "We should as well account for the symmetry of the molecule, that is, the molecule is exactly the same upon exchange of two identical atoms (in this case the hydrogen atoms) and thus all its properties. This can be circumvented by exchanging identical atoms for each new geometry and saving this to the training set as well, which will lead to a larger but symmetrically consistent dataset.\n",
    "\n",
    "\n",
    "### Note: I have quick access to some files and datasets (such as the Structs.xyz file which contains the geometries of these structures) because I recently started to do some work alongside G.E. Moyano, but this has (yet) nothing to do with neural networks so this work applies for the Machine Learning for Physicists course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import psi4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>m10</th>\n",
       "      <td>0.018902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>-0.227835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.142721</td>\n",
       "      <td>0.230513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.966525</td>\n",
       "      <td>9.074669</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.712857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m11</th>\n",
       "      <td>12.285352</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.190886</td>\n",
       "      <td>12.574280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.136145</td>\n",
       "      <td>-0.039439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.055729</td>\n",
       "      <td>0.152842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.657169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.093536</td>\n",
       "      <td>0.201882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.174882</td>\n",
       "      <td>0.927414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.034873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.217334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.819430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.733547</td>\n",
       "      <td>0.869755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.691461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m2</th>\n",
       "      <td>-0.022028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.335390</td>\n",
       "      <td>0.063013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115731</td>\n",
       "      <td>1.009366</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.441728</td>\n",
       "      <td>-0.937213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.710607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0    1         2          3    4         5         6    7   \\\n",
       "m10   0.018902  0.0  0.007275  -0.227835  0.0  1.142721  0.230513  0.0   \n",
       "m11  12.285352  0.0 -4.190886  12.574280  0.0 -3.136145 -0.039439  0.0   \n",
       "m12   0.000000  0.0  0.000000   0.000000  0.0  1.093536  0.201882  0.0   \n",
       "m1    0.000000  0.0  1.217334   0.000000  0.0  0.000000  0.819430  0.0   \n",
       "m2   -0.022028  0.0  1.335390   0.063013  0.0  0.115731  1.009366  0.0   \n",
       "\n",
       "           8         9    10         11  \n",
       "m10 -0.966525  9.074669  0.0 -17.712857  \n",
       "m11 -0.055729  0.152842  0.0   0.657169  \n",
       "m12 -3.174882  0.927414  0.0  -3.034873  \n",
       "m1  -0.733547  0.869755  0.0   1.691461  \n",
       "m2  -0.441728 -0.937213  0.0   1.710607  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atoms = ['O','C','H1','H2']\n",
    "#Now actually read the Structs file, compute distances and fill df\n",
    "\n",
    "def loadCoords(file='../Data/Structs.xyz'):\n",
    "    #Empty df for holding training examples\n",
    "    df = pd.DataFrame({})\n",
    "    \n",
    "    with open(file,'r') as file:\n",
    "        line = 'True'\n",
    "        struct = pd.DataFrame({})\n",
    "        h_index = 1  #Index of the H atom, so as to distinguish both atoms\n",
    "        while line:\n",
    "            line = file.readline()\n",
    "            if not line: #If end of line, end loop\n",
    "                break \n",
    "\n",
    "            if line[-5:].strip() == '.log':\n",
    "                name = line[:-5]\n",
    "                #Restart struct df\n",
    "                struct = pd.DataFrame({})\n",
    "\n",
    "            elif line[0]!= '\\n': #If line not a name, then read structures\n",
    "                line = line.split(' ')\n",
    "                atom = line[0] #atom name\n",
    "\n",
    "                #Distinguish between the two H atoms: rename as H1 and H2\n",
    "                if atom == 'H':\n",
    "                    atom = atom + str(h_index)\n",
    "                    if h_index == 1: h_index += 1\n",
    "                    else: h_index = 1\n",
    "\n",
    "                #Read coordinates and append to struct df\n",
    "                coords = pd.Series(line[1:],name=atom).astype(float)\n",
    "                struct=struct.append(coords)\n",
    "\n",
    "                if struct.shape[0] == 4:  #If reading atoms is done, do computations\n",
    "                    struct = struct.loc[atoms] #Reorder atoms\n",
    "                    df = df.append(pd.Series(struct.values.ravel(),name=name))\n",
    "        return df\n",
    "\n",
    "#DF columns are: (x,y,z) coords for O,C,H1,H2, in that order (Ox, Oy, Oz, Cx, ...)\n",
    "df = loadCoords()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute energy for these initial configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-113.53396156423004"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def HFenergy(row):\n",
    "    psi4.set_memory('500 MB')\n",
    "\n",
    "    #charge = +1, spin multipicity = 2\n",
    "    h2o = psi4.geometry(\"\"\"\n",
    "    1 2\n",
    "    O {} {} {}\n",
    "    C {} {} {} \n",
    "    H {} {} {}\n",
    "    H {} {} {}\n",
    "    \"\"\".format(*row))\n",
    "    \n",
    "    psi4.set_options({'reference': 'uhf'})\n",
    "    \n",
    "    #Energy calculated using UHF/cc-pVDZ\n",
    "    try:\n",
    "        return psi4.energy('scf/cc-pvdz') \n",
    "    except:  #In case there's a convergence error or alike\n",
    "        return 0.0\n",
    "\n",
    "HFenergy(df.loc['m4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.4 s, sys: 710 ms, total: 23.1 s\n",
      "Wall time: 24.7 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>energy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>m10</th>\n",
       "      <td>0.018902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>-0.227835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.142721</td>\n",
       "      <td>0.230513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.966525</td>\n",
       "      <td>9.074669</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.712857</td>\n",
       "      <td>-113.424919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m11</th>\n",
       "      <td>12.285352</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.190886</td>\n",
       "      <td>12.574280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.136145</td>\n",
       "      <td>-0.039439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.055729</td>\n",
       "      <td>0.152842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.657169</td>\n",
       "      <td>-113.401075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.093536</td>\n",
       "      <td>0.201882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.174882</td>\n",
       "      <td>0.927414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.034873</td>\n",
       "      <td>-113.401458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.217334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.819430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.733547</td>\n",
       "      <td>0.869755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.691461</td>\n",
       "      <td>-113.514051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m2</th>\n",
       "      <td>-0.022028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.335390</td>\n",
       "      <td>0.063013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115731</td>\n",
       "      <td>1.009366</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.441728</td>\n",
       "      <td>-0.937213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.710607</td>\n",
       "      <td>-113.519105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0    1         2          3    4         5         6    7  \\\n",
       "m10   0.018902  0.0  0.007275  -0.227835  0.0  1.142721  0.230513  0.0   \n",
       "m11  12.285352  0.0 -4.190886  12.574280  0.0 -3.136145 -0.039439  0.0   \n",
       "m12   0.000000  0.0  0.000000   0.000000  0.0  1.093536  0.201882  0.0   \n",
       "m1    0.000000  0.0  1.217334   0.000000  0.0  0.000000  0.819430  0.0   \n",
       "m2   -0.022028  0.0  1.335390   0.063013  0.0  0.115731  1.009366  0.0   \n",
       "\n",
       "            8         9   10         11      energy  \n",
       "m10 -0.966525  9.074669  0.0 -17.712857 -113.424919  \n",
       "m11 -0.055729  0.152842  0.0   0.657169 -113.401075  \n",
       "m12 -3.174882  0.927414  0.0  -3.034873 -113.401458  \n",
       "m1  -0.733547  0.869755  0.0   1.691461 -113.514051  \n",
       "m2  -0.441728 -0.937213  0.0   1.710607 -113.519105  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df['energy'] = df.apply(HFenergy,axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some random examples. I will start with a dataset of about 1500 examples (1500 gave a converged PES on the referenced work). As said, I'll loop, randomly pick some configuration and add some small noise to its coordinates so as to generate new examples around the given config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>-0.049659</td>\n",
       "      <td>-0.064323</td>\n",
       "      <td>-0.109526</td>\n",
       "      <td>-0.024745</td>\n",
       "      <td>0.063710</td>\n",
       "      <td>1.165070</td>\n",
       "      <td>3.304185</td>\n",
       "      <td>0.010910</td>\n",
       "      <td>2.143733</td>\n",
       "      <td>-0.078837</td>\n",
       "      <td>-0.119198</td>\n",
       "      <td>-1.017430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>0.044281</td>\n",
       "      <td>-0.082365</td>\n",
       "      <td>-0.029333</td>\n",
       "      <td>0.101443</td>\n",
       "      <td>-0.034725</td>\n",
       "      <td>0.995798</td>\n",
       "      <td>3.096817</td>\n",
       "      <td>0.011319</td>\n",
       "      <td>2.028340</td>\n",
       "      <td>-0.082419</td>\n",
       "      <td>-0.078600</td>\n",
       "      <td>-1.145300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>-0.253839</td>\n",
       "      <td>-0.145243</td>\n",
       "      <td>0.012894</td>\n",
       "      <td>-0.145618</td>\n",
       "      <td>0.027589</td>\n",
       "      <td>1.230593</td>\n",
       "      <td>2.975962</td>\n",
       "      <td>0.032645</td>\n",
       "      <td>1.914966</td>\n",
       "      <td>0.095536</td>\n",
       "      <td>-0.036148</td>\n",
       "      <td>-0.845204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>-0.195797</td>\n",
       "      <td>-0.073045</td>\n",
       "      <td>-0.051875</td>\n",
       "      <td>-0.044841</td>\n",
       "      <td>-0.028806</td>\n",
       "      <td>1.243050</td>\n",
       "      <td>2.877076</td>\n",
       "      <td>0.069766</td>\n",
       "      <td>1.909304</td>\n",
       "      <td>0.149506</td>\n",
       "      <td>-0.006230</td>\n",
       "      <td>-1.010371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>-0.014415</td>\n",
       "      <td>0.058927</td>\n",
       "      <td>0.276086</td>\n",
       "      <td>-0.129691</td>\n",
       "      <td>-0.048139</td>\n",
       "      <td>1.308764</td>\n",
       "      <td>2.899698</td>\n",
       "      <td>-0.007181</td>\n",
       "      <td>1.984043</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.026127</td>\n",
       "      <td>-1.184842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "1520 -0.049659 -0.064323 -0.109526 -0.024745  0.063710  1.165070  3.304185   \n",
       "1521  0.044281 -0.082365 -0.029333  0.101443 -0.034725  0.995798  3.096817   \n",
       "1522 -0.253839 -0.145243  0.012894 -0.145618  0.027589  1.230593  2.975962   \n",
       "1523 -0.195797 -0.073045 -0.051875 -0.044841 -0.028806  1.243050  2.877076   \n",
       "1524 -0.014415  0.058927  0.276086 -0.129691 -0.048139  1.308764  2.899698   \n",
       "\n",
       "            7         8         9         10        11  \n",
       "1520  0.010910  2.143733 -0.078837 -0.119198 -1.017430  \n",
       "1521  0.011319  2.028340 -0.082419 -0.078600 -1.145300  \n",
       "1522  0.032645  1.914966  0.095536 -0.036148 -0.845204  \n",
       "1523  0.069766  1.909304  0.149506 -0.006230 -1.010371  \n",
       "1524 -0.007181  1.984043  0.001126  0.026127 -1.184842  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df.copy()#.drop(columns='energy')\n",
    "\n",
    "#Loop through the initial configurations and create new example\n",
    "Nsamples = 1500\n",
    "\n",
    "NperStruct = int(Nsamples/df.shape[0])\n",
    "for struct in df.index:\n",
    "    OrigCoords = dataset.loc[struct]\n",
    "    for j in range(NperStruct):\n",
    "        newExamp = OrigCoords + np.random.normal(loc=0,scale=0.1,size=12)\n",
    "        dataset = dataset.append(newExamp)\n",
    "        \n",
    "dataset.reset_index(drop=True,inplace=True)\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22min 52s, sys: 48.7 s, total: 23min 41s\n",
      "Wall time: 25min 20s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>energy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>-0.227835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.142721</td>\n",
       "      <td>0.230513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.966525</td>\n",
       "      <td>9.074669</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.712857</td>\n",
       "      <td>-113.424919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.285352</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.190886</td>\n",
       "      <td>12.574280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.136145</td>\n",
       "      <td>-0.039439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.055729</td>\n",
       "      <td>0.152842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.657169</td>\n",
       "      <td>-113.401075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.093536</td>\n",
       "      <td>0.201882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.174882</td>\n",
       "      <td>0.927414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.034873</td>\n",
       "      <td>-113.401458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.217334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.819430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.733547</td>\n",
       "      <td>0.869755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.691461</td>\n",
       "      <td>-113.514051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.022028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.335390</td>\n",
       "      <td>0.063013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115731</td>\n",
       "      <td>1.009366</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.441728</td>\n",
       "      <td>-0.937213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.710607</td>\n",
       "      <td>-113.519105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0    1         2          3    4         5         6    7  \\\n",
       "0   0.018902  0.0  0.007275  -0.227835  0.0  1.142721  0.230513  0.0   \n",
       "1  12.285352  0.0 -4.190886  12.574280  0.0 -3.136145 -0.039439  0.0   \n",
       "2   0.000000  0.0  0.000000   0.000000  0.0  1.093536  0.201882  0.0   \n",
       "3   0.000000  0.0  1.217334   0.000000  0.0  0.000000  0.819430  0.0   \n",
       "4  -0.022028  0.0  1.335390   0.063013  0.0  0.115731  1.009366  0.0   \n",
       "\n",
       "          8         9   10         11      energy  \n",
       "0 -0.966525  9.074669  0.0 -17.712857 -113.424919  \n",
       "1 -0.055729  0.152842  0.0   0.657169 -113.401075  \n",
       "2 -3.174882  0.927414  0.0  -3.034873 -113.401458  \n",
       "3 -0.733547  0.869755  0.0   1.691461 -113.514051  \n",
       "4 -0.441728 -0.937213  0.0   1.710607 -113.519105  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#Finally, calculate the energies for these configurations\n",
    "dataset['energy'] = dataset.apply(HFenergy,axis=1)\n",
    "\n",
    "#Select all configurations that didn't return an error\n",
    "dataset = dataset[dataset.energy != 0.0]\n",
    "dataset.to_csv('./newEDF.csv',index=False)  #Save\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>energy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>-0.227835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.142721</td>\n",
       "      <td>0.230513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.966525</td>\n",
       "      <td>9.074669</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.712857</td>\n",
       "      <td>-113.424919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.285352</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.190886</td>\n",
       "      <td>12.574280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.136145</td>\n",
       "      <td>-0.039439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.055729</td>\n",
       "      <td>0.152842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.657169</td>\n",
       "      <td>-113.401075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.093536</td>\n",
       "      <td>0.201882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.174882</td>\n",
       "      <td>0.927414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.034873</td>\n",
       "      <td>-113.401458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.217334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.819430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.733547</td>\n",
       "      <td>0.869755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.691461</td>\n",
       "      <td>-113.514051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.022028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.335390</td>\n",
       "      <td>0.063013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115731</td>\n",
       "      <td>1.009366</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.441728</td>\n",
       "      <td>-0.937213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.710607</td>\n",
       "      <td>-113.519105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0    1         2          3    4         5         6    7  \\\n",
       "0   0.018902  0.0  0.007275  -0.227835  0.0  1.142721  0.230513  0.0   \n",
       "1  12.285352  0.0 -4.190886  12.574280  0.0 -3.136145 -0.039439  0.0   \n",
       "2   0.000000  0.0  0.000000   0.000000  0.0  1.093536  0.201882  0.0   \n",
       "3   0.000000  0.0  1.217334   0.000000  0.0  0.000000  0.819430  0.0   \n",
       "4  -0.022028  0.0  1.335390   0.063013  0.0  0.115731  1.009366  0.0   \n",
       "\n",
       "          8         9   10         11      energy  \n",
       "0 -0.966525  9.074669  0.0 -17.712857 -113.424919  \n",
       "1 -0.055729  0.152842  0.0   0.657169 -113.401075  \n",
       "2 -3.174882  0.927414  0.0  -3.034873 -113.401458  \n",
       "3 -0.733547  0.869755  0.0   1.691461 -113.514051  \n",
       "4 -0.441728 -0.937213  0.0   1.710607 -113.519105  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('../Data/newEDF.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>energy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>-0.227835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.142721</td>\n",
       "      <td>9.074669</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.712857</td>\n",
       "      <td>0.230513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.966525</td>\n",
       "      <td>-113.424919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.285352</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.190886</td>\n",
       "      <td>12.574280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.136145</td>\n",
       "      <td>0.152842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.657169</td>\n",
       "      <td>-0.039439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.055729</td>\n",
       "      <td>-113.401075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.093536</td>\n",
       "      <td>0.927414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.034873</td>\n",
       "      <td>0.201882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.174882</td>\n",
       "      <td>-113.401458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.217334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.869755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.691461</td>\n",
       "      <td>0.819430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.733547</td>\n",
       "      <td>-113.514051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.022028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.335390</td>\n",
       "      <td>0.063013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115731</td>\n",
       "      <td>-0.937213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.710607</td>\n",
       "      <td>1.009366</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.441728</td>\n",
       "      <td>-113.519105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0    1         2          3    4         5         6    7  \\\n",
       "0   0.018902  0.0  0.007275  -0.227835  0.0  1.142721  9.074669  0.0   \n",
       "1  12.285352  0.0 -4.190886  12.574280  0.0 -3.136145  0.152842  0.0   \n",
       "2   0.000000  0.0  0.000000   0.000000  0.0  1.093536  0.927414  0.0   \n",
       "3   0.000000  0.0  1.217334   0.000000  0.0  0.000000  0.869755  0.0   \n",
       "4  -0.022028  0.0  1.335390   0.063013  0.0  0.115731 -0.937213  0.0   \n",
       "\n",
       "           8         9   10        11      energy  \n",
       "0 -17.712857  0.230513  0.0 -0.966525 -113.424919  \n",
       "1   0.657169 -0.039439  0.0 -0.055729 -113.401075  \n",
       "2  -3.034873  0.201882  0.0 -3.174882 -113.401458  \n",
       "3   1.691461  0.819430  0.0 -0.733547 -113.514051  \n",
       "4   1.710607  1.009366  0.0 -0.441728 -113.519105  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Permutational symmetry considerations: energy is the same upon H exchange\n",
    "#O,C,H1,H2 --> O,C,H2,H1\n",
    "newdf = dataset.copy()\n",
    "h1 = newdf.loc[:,['6','7','8']].copy()\n",
    "h2 = newdf.loc[:,['9','10','11']].copy()\n",
    "\n",
    "newdf.loc[:,['6','7','8']] = h2.values\n",
    "newdf.loc[:,['9','10','11']] = h1.values\n",
    "\n",
    "newdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2950, 13)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.concat([dataset,newdf])\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/andres/anaconda3/envs/psi4/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0.860626\n",
       "1    1.003485\n",
       "2    0.050251\n",
       "3    0.463291\n",
       "4    0.047561\n",
       "5    0.052803\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getInput(xyz):\n",
    "    \"\"\"xyz: vector containing xyz coords of each atom [Ox, Oy, Oz, Cx, ...]\n",
    "    pass xyz as tf.Variable(xyz)\"\"\"\n",
    "    xyzM = tf.reshape(xyz,(4,1,3))\n",
    "    xyzT = tf.transpose(xyzM,perm=[1,0,2])\n",
    "\n",
    "    DistMatr = tf.sqrt(tf.reduce_sum(tf.square(xyzM-xyzT),axis=2))\n",
    "\n",
    "    mask = np.zeros((4,4))\n",
    "    mask[np.triu_indices(4,1)] = 1\n",
    "\n",
    "    inps = 1./tf.boolean_mask(DistMatr,mask)\n",
    "    return pd.Series(tf.reshape(inps,(-1,6)).numpy()[0])\n",
    "\n",
    "getInput(dataset.iloc[0,:-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "featDF = dataset.drop(columns=['energy']).apply(getInput,axis=1)\n",
    "featDF['energy'] = dataset.energy\n",
    "\n",
    "columns = []\n",
    "#Same loops\n",
    "for i in range(len(atoms)):\n",
    "    for j in range(i+1,len(atoms)):  #Only atoms after ith (not counting it)\n",
    "        columns.append(atoms[i]+atoms[j])\n",
    "featDF.columns = columns + ['energy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'featDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-34620a32e44c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'featDF' is not defined"
     ]
    }
   ],
   "source": [
    "featDF.to_csv(\"./train.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I'm not using train/test split since we're testing using classical dynamics. The error can be measured with a validation set but this inside the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OC</th>\n",
       "      <th>OH1</th>\n",
       "      <th>OH2</th>\n",
       "      <th>CH1</th>\n",
       "      <th>CH2</th>\n",
       "      <th>H1H2</th>\n",
       "      <th>energy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.860626</td>\n",
       "      <td>1.003485</td>\n",
       "      <td>0.050251</td>\n",
       "      <td>0.463291</td>\n",
       "      <td>0.047561</td>\n",
       "      <td>0.052803</td>\n",
       "      <td>0.109043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.914412</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076539</td>\n",
       "      <td>0.077015</td>\n",
       "      <td>0.076996</td>\n",
       "      <td>1.354328</td>\n",
       "      <td>0.132886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.914465</td>\n",
       "      <td>0.314337</td>\n",
       "      <td>0.315118</td>\n",
       "      <td>0.234017</td>\n",
       "      <td>0.236334</td>\n",
       "      <td>1.353331</td>\n",
       "      <td>0.132504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.821467</td>\n",
       "      <td>0.472593</td>\n",
       "      <td>1.009499</td>\n",
       "      <td>0.909257</td>\n",
       "      <td>0.525769</td>\n",
       "      <td>0.412281</td>\n",
       "      <td>0.019911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.817916</td>\n",
       "      <td>0.486682</td>\n",
       "      <td>1.011003</td>\n",
       "      <td>0.910467</td>\n",
       "      <td>0.531188</td>\n",
       "      <td>0.344588</td>\n",
       "      <td>0.014856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         OC       OH1       OH2       CH1       CH2      H1H2    energy\n",
       "0  0.860626  1.003485  0.050251  0.463291  0.047561  0.052803  0.109043\n",
       "1  0.914412  0.076923  0.076539  0.077015  0.076996  1.354328  0.132886\n",
       "2  0.914465  0.314337  0.315118  0.234017  0.236334  1.353331  0.132504\n",
       "3  0.821467  0.472593  1.009499  0.909257  0.525769  0.412281  0.019911\n",
       "4  0.817916  0.486682  1.011003  0.910467  0.531188  0.344588  0.014856"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#then train = featDF\n",
    "train = pd.read_csv('./train.csv')\n",
    "\n",
    "minE = train.energy.min()\n",
    "train.energy = (train.energy - minE)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Up to this point I've been creating and cleaning the dataset. Now let me train a simple NN on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    model = keras.Sequential(\n",
    "        [keras.Input(shape=(6,)),\n",
    "         Dense(254,activation='relu'),\n",
    "         #Dense(64,activation='relu'),\n",
    "         Dense(128,activation='relu'),\n",
    "         Dense(128,activation='relu'),\n",
    "         Dense(64,activation='relu'),\n",
    "         Dense(1,activation='linear')]\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer='adam',loss='mae')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2507 samples, validate on 443 samples\n",
      "Epoch 1/500\n",
      "2507/2507 [==============================] - 1s 224us/sample - loss: 0.0557 - val_loss: 0.0425\n",
      "Epoch 2/500\n",
      "2507/2507 [==============================] - 0s 98us/sample - loss: 0.0477 - val_loss: 0.0557\n",
      "Epoch 3/500\n",
      "2507/2507 [==============================] - 0s 90us/sample - loss: 0.0421 - val_loss: 0.0551\n",
      "Epoch 4/500\n",
      "2507/2507 [==============================] - 0s 95us/sample - loss: 0.0397 - val_loss: 0.0432\n",
      "Epoch 5/500\n",
      "2507/2507 [==============================] - 0s 88us/sample - loss: 0.0361 - val_loss: 0.0446\n",
      "Epoch 6/500\n",
      "2507/2507 [==============================] - 0s 128us/sample - loss: 0.0331 - val_loss: 0.0364\n",
      "Epoch 7/500\n",
      "2507/2507 [==============================] - 0s 121us/sample - loss: 0.0322 - val_loss: 0.0381\n",
      "Epoch 8/500\n",
      "2507/2507 [==============================] - 0s 88us/sample - loss: 0.0308 - val_loss: 0.0371\n",
      "Epoch 9/500\n",
      "2507/2507 [==============================] - 0s 106us/sample - loss: 0.0298 - val_loss: 0.0333\n",
      "Epoch 10/500\n",
      "2507/2507 [==============================] - 0s 86us/sample - loss: 0.0278 - val_loss: 0.0373\n",
      "Epoch 11/500\n",
      "2507/2507 [==============================] - 0s 86us/sample - loss: 0.0253 - val_loss: 0.0393\n",
      "Epoch 12/500\n",
      "2507/2507 [==============================] - 0s 106us/sample - loss: 0.0234 - val_loss: 0.0344\n",
      "Epoch 13/500\n",
      "2507/2507 [==============================] - 0s 126us/sample - loss: 0.0259 - val_loss: 0.0307\n",
      "Epoch 14/500\n",
      "2507/2507 [==============================] - 0s 97us/sample - loss: 0.0197 - val_loss: 0.0277\n",
      "Epoch 15/500\n",
      "2507/2507 [==============================] - 0s 91us/sample - loss: 0.0208 - val_loss: 0.0305\n",
      "Epoch 16/500\n",
      "2507/2507 [==============================] - 0s 95us/sample - loss: 0.0191 - val_loss: 0.0344\n",
      "Epoch 17/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0175 - val_loss: 0.0302\n",
      "Epoch 18/500\n",
      "2507/2507 [==============================] - 0s 124us/sample - loss: 0.0180 - val_loss: 0.0287\n",
      "Epoch 19/500\n",
      "2507/2507 [==============================] - 0s 107us/sample - loss: 0.0167 - val_loss: 0.0356\n",
      "Epoch 20/500\n",
      "2507/2507 [==============================] - 0s 132us/sample - loss: 0.0166 - val_loss: 0.0242\n",
      "Epoch 21/500\n",
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0141 - val_loss: 0.0190\n",
      "Epoch 22/500\n",
      "2507/2507 [==============================] - 0s 87us/sample - loss: 0.0134 - val_loss: 0.0286\n",
      "Epoch 23/500\n",
      "2507/2507 [==============================] - 0s 109us/sample - loss: 0.0140 - val_loss: 0.0250\n",
      "Epoch 24/500\n",
      "2507/2507 [==============================] - 0s 112us/sample - loss: 0.0138 - val_loss: 0.0284\n",
      "Epoch 25/500\n",
      "2507/2507 [==============================] - 0s 89us/sample - loss: 0.0143 - val_loss: 0.0247\n",
      "Epoch 26/500\n",
      "2507/2507 [==============================] - 0s 102us/sample - loss: 0.0136 - val_loss: 0.0259\n",
      "Epoch 27/500\n",
      "2507/2507 [==============================] - 0s 93us/sample - loss: 0.0120 - val_loss: 0.0212\n",
      "Epoch 28/500\n",
      "2507/2507 [==============================] - 0s 127us/sample - loss: 0.0139 - val_loss: 0.0189\n",
      "Epoch 29/500\n",
      "2507/2507 [==============================] - 0s 96us/sample - loss: 0.0126 - val_loss: 0.0233\n",
      "Epoch 30/500\n",
      "2507/2507 [==============================] - 0s 104us/sample - loss: 0.0114 - val_loss: 0.0210\n",
      "Epoch 31/500\n",
      "2507/2507 [==============================] - 0s 110us/sample - loss: 0.0106 - val_loss: 0.0178\n",
      "Epoch 32/500\n",
      "2507/2507 [==============================] - 0s 98us/sample - loss: 0.0123 - val_loss: 0.0222\n",
      "Epoch 33/500\n",
      "2507/2507 [==============================] - 0s 103us/sample - loss: 0.0108 - val_loss: 0.0225\n",
      "Epoch 34/500\n",
      "2507/2507 [==============================] - 0s 106us/sample - loss: 0.0101 - val_loss: 0.0200\n",
      "Epoch 35/500\n",
      "2507/2507 [==============================] - 0s 113us/sample - loss: 0.0114 - val_loss: 0.0260\n",
      "Epoch 36/500\n",
      "2507/2507 [==============================] - 0s 98us/sample - loss: 0.0101 - val_loss: 0.0194\n",
      "Epoch 37/500\n",
      "2507/2507 [==============================] - 0s 104us/sample - loss: 0.0108 - val_loss: 0.0225\n",
      "Epoch 38/500\n",
      "2507/2507 [==============================] - 0s 94us/sample - loss: 0.0105 - val_loss: 0.0262\n",
      "Epoch 39/500\n",
      "2507/2507 [==============================] - 0s 86us/sample - loss: 0.0100 - val_loss: 0.0222\n",
      "Epoch 40/500\n",
      "2507/2507 [==============================] - 0s 107us/sample - loss: 0.0106 - val_loss: 0.0217\n",
      "Epoch 41/500\n",
      "2507/2507 [==============================] - 0s 102us/sample - loss: 0.0099 - val_loss: 0.0217\n",
      "Epoch 42/500\n",
      "2507/2507 [==============================] - 0s 107us/sample - loss: 0.0097 - val_loss: 0.0183\n",
      "Epoch 43/500\n",
      "2507/2507 [==============================] - 0s 94us/sample - loss: 0.0112 - val_loss: 0.0230\n",
      "Epoch 44/500\n",
      "2507/2507 [==============================] - 0s 108us/sample - loss: 0.0110 - val_loss: 0.0243\n",
      "Epoch 45/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0094 - val_loss: 0.0220\n",
      "Epoch 46/500\n",
      "2507/2507 [==============================] - 0s 108us/sample - loss: 0.0090 - val_loss: 0.0194\n",
      "Epoch 47/500\n",
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0107 - val_loss: 0.0225\n",
      "Epoch 48/500\n",
      "2507/2507 [==============================] - 0s 107us/sample - loss: 0.0100 - val_loss: 0.0197\n",
      "Epoch 49/500\n",
      "2507/2507 [==============================] - 0s 89us/sample - loss: 0.0093 - val_loss: 0.0210\n",
      "Epoch 50/500\n",
      "2507/2507 [==============================] - 0s 104us/sample - loss: 0.0091 - val_loss: 0.0238\n",
      "Epoch 51/500\n",
      "2507/2507 [==============================] - 0s 102us/sample - loss: 0.0088 - val_loss: 0.0235\n",
      "Epoch 52/500\n",
      "2507/2507 [==============================] - 0s 111us/sample - loss: 0.0073 - val_loss: 0.0191\n",
      "Epoch 53/500\n",
      "2507/2507 [==============================] - 0s 100us/sample - loss: 0.0069 - val_loss: 0.0180\n",
      "Epoch 54/500\n",
      "2507/2507 [==============================] - 0s 115us/sample - loss: 0.0063 - val_loss: 0.0181\n",
      "Epoch 55/500\n",
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0065 - val_loss: 0.0172\n",
      "Epoch 56/500\n",
      "2507/2507 [==============================] - 0s 112us/sample - loss: 0.0063 - val_loss: 0.0171\n",
      "Epoch 57/500\n",
      "2507/2507 [==============================] - 0s 93us/sample - loss: 0.0057 - val_loss: 0.0175\n",
      "Epoch 58/500\n",
      "2507/2507 [==============================] - 0s 112us/sample - loss: 0.0057 - val_loss: 0.0165\n",
      "Epoch 59/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0061 - val_loss: 0.0179\n",
      "Epoch 60/500\n",
      "2507/2507 [==============================] - 0s 122us/sample - loss: 0.0063 - val_loss: 0.0198\n",
      "Epoch 61/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0067 - val_loss: 0.0189\n",
      "Epoch 62/500\n",
      "2507/2507 [==============================] - 0s 103us/sample - loss: 0.0062 - val_loss: 0.0198\n",
      "Epoch 63/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0052 - val_loss: 0.0179\n",
      "Epoch 64/500\n",
      "2507/2507 [==============================] - 0s 100us/sample - loss: 0.0054 - val_loss: 0.0189\n",
      "Epoch 65/500\n",
      "2507/2507 [==============================] - 0s 110us/sample - loss: 0.0059 - val_loss: 0.0172\n",
      "Epoch 66/500\n",
      "2507/2507 [==============================] - 0s 98us/sample - loss: 0.0064 - val_loss: 0.0173\n",
      "Epoch 67/500\n",
      "2507/2507 [==============================] - 0s 107us/sample - loss: 0.0053 - val_loss: 0.0159\n",
      "Epoch 68/500\n",
      "2507/2507 [==============================] - 0s 114us/sample - loss: 0.0057 - val_loss: 0.0157\n",
      "Epoch 69/500\n",
      "2507/2507 [==============================] - 0s 107us/sample - loss: 0.0056 - val_loss: 0.0182\n",
      "Epoch 70/500\n",
      "2507/2507 [==============================] - 0s 107us/sample - loss: 0.0055 - val_loss: 0.0165\n",
      "Epoch 71/500\n",
      "2507/2507 [==============================] - 0s 133us/sample - loss: 0.0058 - val_loss: 0.0156\n",
      "Epoch 72/500\n",
      "2507/2507 [==============================] - 0s 96us/sample - loss: 0.0054 - val_loss: 0.0168\n",
      "Epoch 73/500\n",
      "2507/2507 [==============================] - 0s 93us/sample - loss: 0.0054 - val_loss: 0.0186\n",
      "Epoch 74/500\n",
      "2507/2507 [==============================] - 0s 98us/sample - loss: 0.0056 - val_loss: 0.0177\n",
      "Epoch 75/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0056 - val_loss: 0.0166\n",
      "Epoch 76/500\n",
      "2507/2507 [==============================] - 0s 107us/sample - loss: 0.0052 - val_loss: 0.0188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/500\n",
      "2507/2507 [==============================] - 0s 98us/sample - loss: 0.0073 - val_loss: 0.0168\n",
      "Epoch 78/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0052 - val_loss: 0.0169\n",
      "Epoch 79/500\n",
      "2507/2507 [==============================] - 0s 104us/sample - loss: 0.0055 - val_loss: 0.0181\n",
      "Epoch 80/500\n",
      "2507/2507 [==============================] - 0s 128us/sample - loss: 0.0054 - val_loss: 0.0176\n",
      "Epoch 81/500\n",
      "2507/2507 [==============================] - 0s 130us/sample - loss: 0.0055 - val_loss: 0.0196\n",
      "Epoch 82/500\n",
      "2507/2507 [==============================] - 0s 90us/sample - loss: 0.0055 - val_loss: 0.0169\n",
      "Epoch 83/500\n",
      "2507/2507 [==============================] - 0s 109us/sample - loss: 0.0054 - val_loss: 0.0173\n",
      "Epoch 84/500\n",
      "2507/2507 [==============================] - 0s 82us/sample - loss: 0.0050 - val_loss: 0.0181\n",
      "Epoch 85/500\n",
      "2507/2507 [==============================] - 0s 87us/sample - loss: 0.0052 - val_loss: 0.0162\n",
      "Epoch 86/500\n",
      "2507/2507 [==============================] - 0s 92us/sample - loss: 0.0058 - val_loss: 0.0168\n",
      "Epoch 87/500\n",
      "2507/2507 [==============================] - 0s 94us/sample - loss: 0.0056 - val_loss: 0.0169\n",
      "Epoch 88/500\n",
      "2507/2507 [==============================] - 0s 88us/sample - loss: 0.0055 - val_loss: 0.0188\n",
      "Epoch 89/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0047 - val_loss: 0.0168\n",
      "Epoch 90/500\n",
      "2507/2507 [==============================] - 0s 105us/sample - loss: 0.0040 - val_loss: 0.0163\n",
      "Epoch 91/500\n",
      "2507/2507 [==============================] - 0s 92us/sample - loss: 0.0037 - val_loss: 0.0165\n",
      "Epoch 92/500\n",
      "2507/2507 [==============================] - 0s 91us/sample - loss: 0.0041 - val_loss: 0.0169\n",
      "Epoch 93/500\n",
      "2507/2507 [==============================] - 0s 109us/sample - loss: 0.0038 - val_loss: 0.0146\n",
      "Epoch 94/500\n",
      "2507/2507 [==============================] - 0s 87us/sample - loss: 0.0038 - val_loss: 0.0155\n",
      "Epoch 95/500\n",
      "2507/2507 [==============================] - 0s 93us/sample - loss: 0.0035 - val_loss: 0.0164\n",
      "Epoch 96/500\n",
      "2507/2507 [==============================] - 0s 75us/sample - loss: 0.0040 - val_loss: 0.0164\n",
      "Epoch 97/500\n",
      "2507/2507 [==============================] - 0s 75us/sample - loss: 0.0039 - val_loss: 0.0159\n",
      "Epoch 98/500\n",
      "2507/2507 [==============================] - 0s 78us/sample - loss: 0.0038 - val_loss: 0.0159\n",
      "Epoch 99/500\n",
      "2507/2507 [==============================] - 0s 88us/sample - loss: 0.0040 - val_loss: 0.0160\n",
      "Epoch 100/500\n",
      "2507/2507 [==============================] - 0s 79us/sample - loss: 0.0039 - val_loss: 0.0165\n",
      "Epoch 101/500\n",
      "2507/2507 [==============================] - 0s 104us/sample - loss: 0.0038 - val_loss: 0.0160\n",
      "Epoch 102/500\n",
      "2507/2507 [==============================] - 0s 100us/sample - loss: 0.0037 - val_loss: 0.0154\n",
      "Epoch 103/500\n",
      "2507/2507 [==============================] - 0s 93us/sample - loss: 0.0035 - val_loss: 0.0157\n",
      "Epoch 104/500\n",
      "2507/2507 [==============================] - 0s 93us/sample - loss: 0.0037 - val_loss: 0.0171\n",
      "Epoch 105/500\n",
      "2507/2507 [==============================] - 0s 96us/sample - loss: 0.0038 - val_loss: 0.0160\n",
      "Epoch 106/500\n",
      "2507/2507 [==============================] - 0s 96us/sample - loss: 0.0037 - val_loss: 0.0152\n",
      "Epoch 107/500\n",
      "2507/2507 [==============================] - 0s 102us/sample - loss: 0.0040 - val_loss: 0.0146\n",
      "Epoch 108/500\n",
      "2507/2507 [==============================] - 0s 84us/sample - loss: 0.0036 - val_loss: 0.0155\n",
      "Epoch 109/500\n",
      "2507/2507 [==============================] - 0s 91us/sample - loss: 0.0037 - val_loss: 0.0159\n",
      "Epoch 110/500\n",
      "2507/2507 [==============================] - 0s 93us/sample - loss: 0.0035 - val_loss: 0.0150\n",
      "Epoch 111/500\n",
      "2507/2507 [==============================] - 0s 100us/sample - loss: 0.0034 - val_loss: 0.0158\n",
      "Epoch 112/500\n",
      "2507/2507 [==============================] - 0s 96us/sample - loss: 0.0035 - val_loss: 0.0149\n",
      "Epoch 113/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0035 - val_loss: 0.0146\n",
      "Epoch 114/500\n",
      "2507/2507 [==============================] - 0s 93us/sample - loss: 0.0029 - val_loss: 0.0150\n",
      "Epoch 115/500\n",
      "2507/2507 [==============================] - 0s 79us/sample - loss: 0.0027 - val_loss: 0.0149\n",
      "Epoch 116/500\n",
      "2507/2507 [==============================] - 0s 78us/sample - loss: 0.0029 - val_loss: 0.0153\n",
      "Epoch 117/500\n",
      "2507/2507 [==============================] - 0s 72us/sample - loss: 0.0028 - val_loss: 0.0148\n",
      "Epoch 118/500\n",
      "2507/2507 [==============================] - 0s 77us/sample - loss: 0.0027 - val_loss: 0.0148\n",
      "Epoch 119/500\n",
      "2507/2507 [==============================] - 0s 74us/sample - loss: 0.0026 - val_loss: 0.0148\n",
      "Epoch 120/500\n",
      "2507/2507 [==============================] - 0s 96us/sample - loss: 0.0027 - val_loss: 0.0143\n",
      "Epoch 121/500\n",
      "2507/2507 [==============================] - 0s 82us/sample - loss: 0.0027 - val_loss: 0.0145\n",
      "Epoch 122/500\n",
      "2507/2507 [==============================] - 0s 82us/sample - loss: 0.0028 - val_loss: 0.0140\n",
      "Epoch 123/500\n",
      "2507/2507 [==============================] - 0s 76us/sample - loss: 0.0027 - val_loss: 0.0143\n",
      "Epoch 124/500\n",
      "2507/2507 [==============================] - 0s 90us/sample - loss: 0.0028 - val_loss: 0.0145\n",
      "Epoch 125/500\n",
      "2507/2507 [==============================] - 0s 85us/sample - loss: 0.0027 - val_loss: 0.0145\n",
      "Epoch 126/500\n",
      "2507/2507 [==============================] - 0s 87us/sample - loss: 0.0026 - val_loss: 0.0151\n",
      "Epoch 127/500\n",
      "2507/2507 [==============================] - 0s 91us/sample - loss: 0.0027 - val_loss: 0.0147\n",
      "Epoch 128/500\n",
      "2507/2507 [==============================] - 0s 92us/sample - loss: 0.0026 - val_loss: 0.0142\n",
      "Epoch 129/500\n",
      "2507/2507 [==============================] - 0s 84us/sample - loss: 0.0027 - val_loss: 0.0142\n",
      "Epoch 130/500\n",
      "2507/2507 [==============================] - 0s 86us/sample - loss: 0.0025 - val_loss: 0.0143\n",
      "Epoch 131/500\n",
      "2507/2507 [==============================] - 0s 85us/sample - loss: 0.0028 - val_loss: 0.0152\n",
      "Epoch 132/500\n",
      "2507/2507 [==============================] - 0s 94us/sample - loss: 0.0027 - val_loss: 0.0144\n",
      "Epoch 133/500\n",
      "2507/2507 [==============================] - 0s 82us/sample - loss: 0.0026 - val_loss: 0.0142\n",
      "Epoch 134/500\n",
      "2507/2507 [==============================] - 0s 89us/sample - loss: 0.0026 - val_loss: 0.0143\n",
      "Epoch 135/500\n",
      "2507/2507 [==============================] - 0s 97us/sample - loss: 0.0024 - val_loss: 0.0146\n",
      "Epoch 136/500\n",
      "2507/2507 [==============================] - 0s 77us/sample - loss: 0.0028 - val_loss: 0.0140\n",
      "Epoch 137/500\n",
      "2507/2507 [==============================] - 0s 87us/sample - loss: 0.0027 - val_loss: 0.0139\n",
      "Epoch 138/500\n",
      "2507/2507 [==============================] - 0s 89us/sample - loss: 0.0026 - val_loss: 0.0142\n",
      "Epoch 139/500\n",
      "2507/2507 [==============================] - 0s 91us/sample - loss: 0.0025 - val_loss: 0.0140\n",
      "Epoch 140/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0025 - val_loss: 0.0136\n",
      "Epoch 141/500\n",
      "2507/2507 [==============================] - 0s 98us/sample - loss: 0.0026 - val_loss: 0.0141\n",
      "Epoch 142/500\n",
      "2507/2507 [==============================] - 0s 111us/sample - loss: 0.0026 - val_loss: 0.0136\n",
      "Epoch 143/500\n",
      "2507/2507 [==============================] - 0s 95us/sample - loss: 0.0026 - val_loss: 0.0139\n",
      "Epoch 144/500\n",
      "2507/2507 [==============================] - 0s 89us/sample - loss: 0.0026 - val_loss: 0.0137\n",
      "Epoch 145/500\n",
      "2507/2507 [==============================] - 0s 89us/sample - loss: 0.0025 - val_loss: 0.0140\n",
      "Epoch 146/500\n",
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0026 - val_loss: 0.0140\n",
      "Epoch 147/500\n",
      "2507/2507 [==============================] - 0s 107us/sample - loss: 0.0026 - val_loss: 0.0135\n",
      "Epoch 148/500\n",
      "2507/2507 [==============================] - 0s 83us/sample - loss: 0.0027 - val_loss: 0.0137\n",
      "Epoch 149/500\n",
      "2507/2507 [==============================] - 0s 93us/sample - loss: 0.0026 - val_loss: 0.0136\n",
      "Epoch 150/500\n",
      "2507/2507 [==============================] - 0s 82us/sample - loss: 0.0025 - val_loss: 0.0137\n",
      "Epoch 151/500\n",
      "2507/2507 [==============================] - 0s 94us/sample - loss: 0.0025 - val_loss: 0.0137\n",
      "Epoch 152/500\n",
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0024 - val_loss: 0.0139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/500\n",
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0024 - val_loss: 0.0139\n",
      "Epoch 154/500\n",
      "2507/2507 [==============================] - 0s 94us/sample - loss: 0.0026 - val_loss: 0.0133\n",
      "Epoch 155/500\n",
      "2507/2507 [==============================] - 0s 81us/sample - loss: 0.0025 - val_loss: 0.0136\n",
      "Epoch 156/500\n",
      "2507/2507 [==============================] - 0s 96us/sample - loss: 0.0024 - val_loss: 0.0135\n",
      "Epoch 157/500\n",
      "2507/2507 [==============================] - 0s 82us/sample - loss: 0.0024 - val_loss: 0.0140\n",
      "Epoch 158/500\n",
      "2507/2507 [==============================] - 0s 77us/sample - loss: 0.0025 - val_loss: 0.0133\n",
      "Epoch 159/500\n",
      "2507/2507 [==============================] - 0s 74us/sample - loss: 0.0024 - val_loss: 0.0136\n",
      "Epoch 160/500\n",
      "2507/2507 [==============================] - 0s 91us/sample - loss: 0.0024 - val_loss: 0.0130\n",
      "Epoch 161/500\n",
      "2507/2507 [==============================] - 0s 82us/sample - loss: 0.0026 - val_loss: 0.0138\n",
      "Epoch 162/500\n",
      "2507/2507 [==============================] - 0s 77us/sample - loss: 0.0024 - val_loss: 0.0133\n",
      "Epoch 163/500\n",
      "2507/2507 [==============================] - 0s 75us/sample - loss: 0.0024 - val_loss: 0.0132\n",
      "Epoch 164/500\n",
      "2507/2507 [==============================] - 0s 90us/sample - loss: 0.0025 - val_loss: 0.0129\n",
      "Epoch 165/500\n",
      "2507/2507 [==============================] - 0s 87us/sample - loss: 0.0023 - val_loss: 0.0138\n",
      "Epoch 166/500\n",
      "2507/2507 [==============================] - 0s 87us/sample - loss: 0.0024 - val_loss: 0.0132\n",
      "Epoch 167/500\n",
      "2507/2507 [==============================] - 0s 93us/sample - loss: 0.0025 - val_loss: 0.0137\n",
      "Epoch 168/500\n",
      "2507/2507 [==============================] - 0s 90us/sample - loss: 0.0025 - val_loss: 0.0135\n",
      "Epoch 169/500\n",
      "2507/2507 [==============================] - 0s 90us/sample - loss: 0.0026 - val_loss: 0.0132\n",
      "Epoch 170/500\n",
      "2507/2507 [==============================] - 0s 97us/sample - loss: 0.0024 - val_loss: 0.0133\n",
      "Epoch 171/500\n",
      "2507/2507 [==============================] - 0s 90us/sample - loss: 0.0026 - val_loss: 0.0134\n",
      "Epoch 172/500\n",
      "2507/2507 [==============================] - 0s 97us/sample - loss: 0.0025 - val_loss: 0.0132\n",
      "Epoch 173/500\n",
      "2507/2507 [==============================] - 0s 102us/sample - loss: 0.0024 - val_loss: 0.0128\n",
      "Epoch 174/500\n",
      "2507/2507 [==============================] - 0s 88us/sample - loss: 0.0024 - val_loss: 0.0130\n",
      "Epoch 175/500\n",
      "2507/2507 [==============================] - 0s 91us/sample - loss: 0.0023 - val_loss: 0.0130\n",
      "Epoch 176/500\n",
      "2507/2507 [==============================] - 0s 88us/sample - loss: 0.0024 - val_loss: 0.0129\n",
      "Epoch 177/500\n",
      "2507/2507 [==============================] - 0s 95us/sample - loss: 0.0023 - val_loss: 0.0135\n",
      "Epoch 178/500\n",
      "2507/2507 [==============================] - 0s 107us/sample - loss: 0.0023 - val_loss: 0.0132\n",
      "Epoch 179/500\n",
      "2507/2507 [==============================] - 0s 105us/sample - loss: 0.0025 - val_loss: 0.0130\n",
      "Epoch 180/500\n",
      "2507/2507 [==============================] - 0s 94us/sample - loss: 0.0024 - val_loss: 0.0131\n",
      "Epoch 181/500\n",
      "2507/2507 [==============================] - 0s 122us/sample - loss: 0.0025 - val_loss: 0.0128\n",
      "Epoch 182/500\n",
      "2507/2507 [==============================] - 0s 107us/sample - loss: 0.0024 - val_loss: 0.0126\n",
      "Epoch 183/500\n",
      "2507/2507 [==============================] - 0s 74us/sample - loss: 0.0023 - val_loss: 0.0129\n",
      "Epoch 184/500\n",
      "2507/2507 [==============================] - 0s 81us/sample - loss: 0.0024 - val_loss: 0.0123\n",
      "Epoch 185/500\n",
      "2507/2507 [==============================] - 0s 83us/sample - loss: 0.0024 - val_loss: 0.0122\n",
      "Epoch 186/500\n",
      "2507/2507 [==============================] - 0s 76us/sample - loss: 0.0024 - val_loss: 0.0126\n",
      "Epoch 187/500\n",
      "2507/2507 [==============================] - 0s 77us/sample - loss: 0.0023 - val_loss: 0.0129\n",
      "Epoch 188/500\n",
      "2507/2507 [==============================] - 0s 78us/sample - loss: 0.0023 - val_loss: 0.0131\n",
      "Epoch 189/500\n",
      "2507/2507 [==============================] - 0s 95us/sample - loss: 0.0023 - val_loss: 0.0132\n",
      "Epoch 190/500\n",
      "2507/2507 [==============================] - 0s 89us/sample - loss: 0.0023 - val_loss: 0.0125\n",
      "Epoch 191/500\n",
      "2507/2507 [==============================] - 0s 82us/sample - loss: 0.0026 - val_loss: 0.0124\n",
      "Epoch 192/500\n",
      "2507/2507 [==============================] - 0s 87us/sample - loss: 0.0025 - val_loss: 0.0128\n",
      "Epoch 193/500\n",
      "2507/2507 [==============================] - 0s 86us/sample - loss: 0.0024 - val_loss: 0.0129\n",
      "Epoch 194/500\n",
      "2507/2507 [==============================] - 0s 100us/sample - loss: 0.0023 - val_loss: 0.0127\n",
      "Epoch 195/500\n",
      "2507/2507 [==============================] - 0s 84us/sample - loss: 0.0022 - val_loss: 0.0124\n",
      "Epoch 196/500\n",
      "2507/2507 [==============================] - 0s 81us/sample - loss: 0.0024 - val_loss: 0.0128\n",
      "Epoch 197/500\n",
      "2507/2507 [==============================] - 0s 76us/sample - loss: 0.0024 - val_loss: 0.0126\n",
      "Epoch 198/500\n",
      "2507/2507 [==============================] - 0s 80us/sample - loss: 0.0023 - val_loss: 0.0128\n",
      "Epoch 199/500\n",
      "2507/2507 [==============================] - 0s 84us/sample - loss: 0.0023 - val_loss: 0.0126\n",
      "Epoch 200/500\n",
      "2507/2507 [==============================] - 0s 92us/sample - loss: 0.0024 - val_loss: 0.0121\n",
      "Epoch 201/500\n",
      "2507/2507 [==============================] - 0s 96us/sample - loss: 0.0023 - val_loss: 0.0125\n",
      "Epoch 202/500\n",
      "2507/2507 [==============================] - 0s 92us/sample - loss: 0.0022 - val_loss: 0.0122\n",
      "Epoch 203/500\n",
      "2507/2507 [==============================] - 0s 88us/sample - loss: 0.0023 - val_loss: 0.0123\n",
      "Epoch 204/500\n",
      "2507/2507 [==============================] - 0s 88us/sample - loss: 0.0024 - val_loss: 0.0126\n",
      "Epoch 205/500\n",
      "2507/2507 [==============================] - 0s 84us/sample - loss: 0.0023 - val_loss: 0.0123\n",
      "Epoch 206/500\n",
      "2507/2507 [==============================] - 0s 91us/sample - loss: 0.0023 - val_loss: 0.0124\n",
      "Epoch 207/500\n",
      "2507/2507 [==============================] - 0s 95us/sample - loss: 0.0024 - val_loss: 0.0127\n",
      "Epoch 208/500\n",
      "2507/2507 [==============================] - 0s 92us/sample - loss: 0.0022 - val_loss: 0.0126\n",
      "Epoch 209/500\n",
      "2507/2507 [==============================] - 0s 130us/sample - loss: 0.0022 - val_loss: 0.0129\n",
      "Epoch 210/500\n",
      "2507/2507 [==============================] - 0s 118us/sample - loss: 0.0023 - val_loss: 0.0122\n",
      "Epoch 211/500\n",
      "2507/2507 [==============================] - 0s 120us/sample - loss: 0.0023 - val_loss: 0.0120\n",
      "Epoch 212/500\n",
      "2507/2507 [==============================] - 0s 114us/sample - loss: 0.0024 - val_loss: 0.0124\n",
      "Epoch 213/500\n",
      "2507/2507 [==============================] - 0s 106us/sample - loss: 0.0023 - val_loss: 0.0127\n",
      "Epoch 214/500\n",
      "2507/2507 [==============================] - 0s 119us/sample - loss: 0.0024 - val_loss: 0.0124\n",
      "Epoch 215/500\n",
      "2507/2507 [==============================] - 0s 126us/sample - loss: 0.0023 - val_loss: 0.0123\n",
      "Epoch 216/500\n",
      "2507/2507 [==============================] - 0s 98us/sample - loss: 0.0022 - val_loss: 0.0127\n",
      "Epoch 217/500\n",
      "2507/2507 [==============================] - 0s 89us/sample - loss: 0.0022 - val_loss: 0.0126\n",
      "Epoch 218/500\n",
      "2507/2507 [==============================] - 0s 97us/sample - loss: 0.0021 - val_loss: 0.0125\n",
      "Epoch 219/500\n",
      "2507/2507 [==============================] - 0s 102us/sample - loss: 0.0021 - val_loss: 0.0120\n",
      "Epoch 220/500\n",
      "2507/2507 [==============================] - 0s 143us/sample - loss: 0.0022 - val_loss: 0.0123\n",
      "Epoch 221/500\n",
      "2507/2507 [==============================] - 0s 102us/sample - loss: 0.0021 - val_loss: 0.0126\n",
      "Epoch 222/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0024 - val_loss: 0.0126\n",
      "Epoch 223/500\n",
      "2507/2507 [==============================] - 0s 105us/sample - loss: 0.0022 - val_loss: 0.0122\n",
      "Epoch 224/500\n",
      "2507/2507 [==============================] - 0s 95us/sample - loss: 0.0022 - val_loss: 0.0121\n",
      "Epoch 225/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0022 - val_loss: 0.0121\n",
      "Epoch 226/500\n",
      "2507/2507 [==============================] - 0s 116us/sample - loss: 0.0023 - val_loss: 0.0117\n",
      "Epoch 227/500\n",
      "2507/2507 [==============================] - 0s 90us/sample - loss: 0.0021 - val_loss: 0.0118\n",
      "Epoch 228/500\n",
      "2507/2507 [==============================] - 0s 93us/sample - loss: 0.0022 - val_loss: 0.0118\n",
      "Epoch 229/500\n",
      "2507/2507 [==============================] - 0s 83us/sample - loss: 0.0022 - val_loss: 0.0119\n",
      "Epoch 230/500\n",
      "2507/2507 [==============================] - 0s 84us/sample - loss: 0.0023 - val_loss: 0.0121\n",
      "Epoch 231/500\n",
      "2507/2507 [==============================] - 0s 106us/sample - loss: 0.0023 - val_loss: 0.0117\n",
      "Epoch 232/500\n",
      "2507/2507 [==============================] - 0s 111us/sample - loss: 0.0022 - val_loss: 0.0119\n",
      "Epoch 233/500\n",
      "2507/2507 [==============================] - 0s 106us/sample - loss: 0.0022 - val_loss: 0.0114\n",
      "Epoch 234/500\n",
      "2507/2507 [==============================] - 0s 97us/sample - loss: 0.0023 - val_loss: 0.0119\n",
      "Epoch 235/500\n",
      "2507/2507 [==============================] - 0s 110us/sample - loss: 0.0024 - val_loss: 0.0119\n",
      "Epoch 236/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0022 - val_loss: 0.0114\n",
      "Epoch 237/500\n",
      "2507/2507 [==============================] - 0s 110us/sample - loss: 0.0022 - val_loss: 0.0114\n",
      "Epoch 238/500\n",
      "2507/2507 [==============================] - 0s 110us/sample - loss: 0.0021 - val_loss: 0.0120\n",
      "Epoch 239/500\n",
      "2507/2507 [==============================] - 0s 106us/sample - loss: 0.0022 - val_loss: 0.0122\n",
      "Epoch 240/500\n",
      "2507/2507 [==============================] - 0s 93us/sample - loss: 0.0025 - val_loss: 0.0120\n",
      "Epoch 241/500\n",
      "2507/2507 [==============================] - 0s 103us/sample - loss: 0.0024 - val_loss: 0.0123\n",
      "Epoch 242/500\n",
      "2507/2507 [==============================] - 0s 98us/sample - loss: 0.0023 - val_loss: 0.0120\n",
      "Epoch 243/500\n",
      "2507/2507 [==============================] - 0s 107us/sample - loss: 0.0022 - val_loss: 0.0120\n",
      "Epoch 244/500\n",
      "2507/2507 [==============================] - 0s 104us/sample - loss: 0.0021 - val_loss: 0.0119\n",
      "Epoch 245/500\n",
      "2507/2507 [==============================] - 0s 110us/sample - loss: 0.0023 - val_loss: 0.0117\n",
      "Epoch 246/500\n",
      "2507/2507 [==============================] - 0s 106us/sample - loss: 0.0022 - val_loss: 0.0119\n",
      "Epoch 247/500\n",
      "2507/2507 [==============================] - 0s 102us/sample - loss: 0.0021 - val_loss: 0.0118\n",
      "Epoch 248/500\n",
      "2507/2507 [==============================] - 0s 110us/sample - loss: 0.0021 - val_loss: 0.0121\n",
      "Epoch 249/500\n",
      "2507/2507 [==============================] - 0s 119us/sample - loss: 0.0022 - val_loss: 0.0117\n",
      "Epoch 250/500\n",
      "2507/2507 [==============================] - 0s 98us/sample - loss: 0.0021 - val_loss: 0.0120\n",
      "Epoch 251/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0023 - val_loss: 0.0121\n",
      "Epoch 252/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0021 - val_loss: 0.0120\n",
      "Epoch 253/500\n",
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0022 - val_loss: 0.0119\n",
      "Epoch 254/500\n",
      "2507/2507 [==============================] - 0s 124us/sample - loss: 0.0018 - val_loss: 0.0121\n",
      "Epoch 255/500\n",
      "2507/2507 [==============================] - 0s 124us/sample - loss: 0.0019 - val_loss: 0.0117\n",
      "Epoch 256/500\n",
      "2507/2507 [==============================] - 0s 103us/sample - loss: 0.0017 - val_loss: 0.0115\n",
      "Epoch 257/500\n",
      "2507/2507 [==============================] - 0s 97us/sample - loss: 0.0017 - val_loss: 0.0116\n",
      "Epoch 258/500\n",
      "2507/2507 [==============================] - 0s 88us/sample - loss: 0.0017 - val_loss: 0.0116\n",
      "Epoch 259/500\n",
      "2507/2507 [==============================] - 0s 100us/sample - loss: 0.0017 - val_loss: 0.0117\n",
      "Epoch 260/500\n",
      "2507/2507 [==============================] - 0s 124us/sample - loss: 0.0017 - val_loss: 0.0119\n",
      "Epoch 261/500\n",
      "2507/2507 [==============================] - 0s 109us/sample - loss: 0.0017 - val_loss: 0.0114\n",
      "Epoch 262/500\n",
      "2507/2507 [==============================] - 0s 97us/sample - loss: 0.0017 - val_loss: 0.0116\n",
      "Epoch 263/500\n",
      "2507/2507 [==============================] - 0s 104us/sample - loss: 0.0018 - val_loss: 0.0116\n",
      "Epoch 264/500\n",
      "2507/2507 [==============================] - 0s 106us/sample - loss: 0.0018 - val_loss: 0.0113\n",
      "Epoch 265/500\n",
      "2507/2507 [==============================] - 0s 104us/sample - loss: 0.0017 - val_loss: 0.0116\n",
      "Epoch 266/500\n",
      "2507/2507 [==============================] - 0s 116us/sample - loss: 0.0016 - val_loss: 0.0116\n",
      "Epoch 267/500\n",
      "2507/2507 [==============================] - 0s 115us/sample - loss: 0.0017 - val_loss: 0.0113\n",
      "Epoch 268/500\n",
      "2507/2507 [==============================] - 0s 98us/sample - loss: 0.0017 - val_loss: 0.0115\n",
      "Epoch 269/500\n",
      "2507/2507 [==============================] - 0s 100us/sample - loss: 0.0017 - val_loss: 0.0117\n",
      "Epoch 270/500\n",
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0017 - val_loss: 0.0114\n",
      "Epoch 271/500\n",
      "2507/2507 [==============================] - 0s 117us/sample - loss: 0.0016 - val_loss: 0.0115\n",
      "Epoch 272/500\n",
      "2507/2507 [==============================] - 0s 119us/sample - loss: 0.0017 - val_loss: 0.0114\n",
      "Epoch 273/500\n",
      "2507/2507 [==============================] - 0s 95us/sample - loss: 0.0017 - val_loss: 0.0115\n",
      "Epoch 274/500\n",
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0017 - val_loss: 0.0115\n",
      "Epoch 275/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0018 - val_loss: 0.0115\n",
      "Epoch 276/500\n",
      "2507/2507 [==============================] - 0s 104us/sample - loss: 0.0018 - val_loss: 0.0117\n",
      "Epoch 277/500\n",
      "2507/2507 [==============================] - 0s 109us/sample - loss: 0.0017 - val_loss: 0.0116\n",
      "Epoch 278/500\n",
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0017 - val_loss: 0.0117\n",
      "Epoch 279/500\n",
      "2507/2507 [==============================] - 0s 121us/sample - loss: 0.0016 - val_loss: 0.0112\n",
      "Epoch 280/500\n",
      "2507/2507 [==============================] - 0s 95us/sample - loss: 0.0017 - val_loss: 0.0113\n",
      "Epoch 281/500\n",
      "2507/2507 [==============================] - 0s 108us/sample - loss: 0.0017 - val_loss: 0.0112\n",
      "Epoch 282/500\n",
      "2507/2507 [==============================] - 0s 106us/sample - loss: 0.0017 - val_loss: 0.0113\n",
      "Epoch 283/500\n",
      "2507/2507 [==============================] - 0s 111us/sample - loss: 0.0017 - val_loss: 0.0119\n",
      "Epoch 284/500\n",
      "2507/2507 [==============================] - 0s 95us/sample - loss: 0.0017 - val_loss: 0.0115\n",
      "Epoch 285/500\n",
      "2507/2507 [==============================] - 0s 102us/sample - loss: 0.0017 - val_loss: 0.0112\n",
      "Epoch 286/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0017 - val_loss: 0.0114\n",
      "Epoch 287/500\n",
      "2507/2507 [==============================] - 0s 102us/sample - loss: 0.0016 - val_loss: 0.0115\n",
      "Epoch 288/500\n",
      "2507/2507 [==============================] - 0s 106us/sample - loss: 0.0016 - val_loss: 0.0114\n",
      "Epoch 289/500\n",
      "2507/2507 [==============================] - 0s 120us/sample - loss: 0.0017 - val_loss: 0.0111\n",
      "Epoch 290/500\n",
      "2507/2507 [==============================] - 0s 94us/sample - loss: 0.0018 - val_loss: 0.0115\n",
      "Epoch 291/500\n",
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0017 - val_loss: 0.0112\n",
      "Epoch 292/500\n",
      "2507/2507 [==============================] - 0s 106us/sample - loss: 0.0017 - val_loss: 0.0113\n",
      "Epoch 293/500\n",
      "2507/2507 [==============================] - ETA: 0s - loss: 0.001 - 0s 102us/sample - loss: 0.0017 - val_loss: 0.0113\n",
      "Epoch 294/500\n",
      "2507/2507 [==============================] - 0s 110us/sample - loss: 0.0017 - val_loss: 0.0115\n",
      "Epoch 295/500\n",
      "2507/2507 [==============================] - 0s 122us/sample - loss: 0.0016 - val_loss: 0.0112\n",
      "Epoch 296/500\n",
      "2507/2507 [==============================] - 0s 113us/sample - loss: 0.0016 - val_loss: 0.0109\n",
      "Epoch 297/500\n",
      "2507/2507 [==============================] - 0s 96us/sample - loss: 0.0017 - val_loss: 0.0113\n",
      "Epoch 298/500\n",
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0017 - val_loss: 0.0115\n",
      "Epoch 299/500\n",
      "2507/2507 [==============================] - 0s 106us/sample - loss: 0.0017 - val_loss: 0.0115\n",
      "Epoch 300/500\n",
      "2507/2507 [==============================] - 0s 128us/sample - loss: 0.0016 - val_loss: 0.0109\n",
      "Epoch 301/500\n",
      "2507/2507 [==============================] - 0s 89us/sample - loss: 0.0016 - val_loss: 0.0111\n",
      "Epoch 302/500\n",
      "2507/2507 [==============================] - 0s 103us/sample - loss: 0.0018 - val_loss: 0.0110\n",
      "Epoch 303/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0017 - val_loss: 0.0116\n",
      "Epoch 304/500\n",
      "2507/2507 [==============================] - 0s 96us/sample - loss: 0.0017 - val_loss: 0.0116\n",
      "Epoch 305/500\n",
      "2507/2507 [==============================] - 0s 110us/sample - loss: 0.0016 - val_loss: 0.0111\n",
      "Epoch 306/500\n",
      "2507/2507 [==============================] - 0s 111us/sample - loss: 0.0018 - val_loss: 0.0112\n",
      "Epoch 307/500\n",
      "2507/2507 [==============================] - 0s 88us/sample - loss: 0.0017 - val_loss: 0.0111\n",
      "Epoch 308/500\n",
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0016 - val_loss: 0.0112\n",
      "Epoch 309/500\n",
      "2507/2507 [==============================] - 0s 93us/sample - loss: 0.0017 - val_loss: 0.0111\n",
      "Epoch 310/500\n",
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0017 - val_loss: 0.0111\n",
      "Epoch 311/500\n",
      "2507/2507 [==============================] - 0s 113us/sample - loss: 0.0016 - val_loss: 0.0111\n",
      "Epoch 312/500\n",
      "2507/2507 [==============================] - 0s 105us/sample - loss: 0.0016 - val_loss: 0.0113\n",
      "Epoch 313/500\n",
      "2507/2507 [==============================] - 0s 98us/sample - loss: 0.0016 - val_loss: 0.0113\n",
      "Epoch 314/500\n",
      "2507/2507 [==============================] - 0s 87us/sample - loss: 0.0016 - val_loss: 0.0111\n",
      "Epoch 315/500\n",
      "2507/2507 [==============================] - 0s 92us/sample - loss: 0.0016 - val_loss: 0.0112\n",
      "Epoch 316/500\n",
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0017 - val_loss: 0.0110\n",
      "Epoch 317/500\n",
      "2507/2507 [==============================] - 0s 107us/sample - loss: 0.0016 - val_loss: 0.0112\n",
      "Epoch 318/500\n",
      "2507/2507 [==============================] - 0s 108us/sample - loss: 0.0014 - val_loss: 0.0111\n",
      "Epoch 319/500\n",
      "2507/2507 [==============================] - 0s 94us/sample - loss: 0.0014 - val_loss: 0.0110\n",
      "Epoch 320/500\n",
      "2507/2507 [==============================] - 0s 96us/sample - loss: 0.0014 - val_loss: 0.0111\n",
      "Epoch 321/500\n",
      "2507/2507 [==============================] - 0s 92us/sample - loss: 0.0014 - val_loss: 0.0112\n",
      "Epoch 322/500\n",
      "2507/2507 [==============================] - 0s 97us/sample - loss: 0.0014 - val_loss: 0.0112\n",
      "Epoch 323/500\n",
      "2507/2507 [==============================] - 0s 103us/sample - loss: 0.0014 - val_loss: 0.0111\n",
      "Epoch 324/500\n",
      "2507/2507 [==============================] - 0s 109us/sample - loss: 0.0014 - val_loss: 0.0111\n",
      "Epoch 325/500\n",
      "2507/2507 [==============================] - 0s 92us/sample - loss: 0.0014 - val_loss: 0.0112\n",
      "Epoch 326/500\n",
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0014 - val_loss: 0.0110\n",
      "Epoch 327/500\n",
      "2507/2507 [==============================] - 0s 88us/sample - loss: 0.0014 - val_loss: 0.0110\n",
      "Epoch 328/500\n",
      "2507/2507 [==============================] - 0s 108us/sample - loss: 0.0014 - val_loss: 0.0111\n",
      "Epoch 329/500\n",
      "2507/2507 [==============================] - 0s 114us/sample - loss: 0.0014 - val_loss: 0.0111\n",
      "Epoch 330/500\n",
      "2507/2507 [==============================] - 0s 108us/sample - loss: 0.0014 - val_loss: 0.0112\n",
      "Epoch 331/500\n",
      "2507/2507 [==============================] - 0s 104us/sample - loss: 0.0014 - val_loss: 0.0111\n",
      "Epoch 332/500\n",
      "2507/2507 [==============================] - 0s 111us/sample - loss: 0.0014 - val_loss: 0.0109\n",
      "Epoch 333/500\n",
      "2507/2507 [==============================] - 0s 94us/sample - loss: 0.0014 - val_loss: 0.0111\n",
      "Epoch 334/500\n",
      "2507/2507 [==============================] - 0s 103us/sample - loss: 0.0014 - val_loss: 0.0111\n",
      "Epoch 335/500\n",
      "2507/2507 [==============================] - 0s 117us/sample - loss: 0.0014 - val_loss: 0.0111\n",
      "Epoch 336/500\n",
      "2507/2507 [==============================] - 0s 98us/sample - loss: 0.0014 - val_loss: 0.0110\n",
      "Epoch 337/500\n",
      "2507/2507 [==============================] - 0s 89us/sample - loss: 0.0013 - val_loss: 0.0110\n",
      "Epoch 338/500\n",
      "2507/2507 [==============================] - 0s 100us/sample - loss: 0.0013 - val_loss: 0.0110\n",
      "Epoch 339/500\n",
      "2507/2507 [==============================] - 0s 96us/sample - loss: 0.0013 - val_loss: 0.0110\n",
      "Epoch 340/500\n",
      "2507/2507 [==============================] - 0s 102us/sample - loss: 0.0013 - val_loss: 0.0110\n",
      "Epoch 341/500\n",
      "2507/2507 [==============================] - 0s 108us/sample - loss: 0.0013 - val_loss: 0.0111\n",
      "Epoch 342/500\n",
      "2507/2507 [==============================] - 0s 107us/sample - loss: 0.0013 - val_loss: 0.0109\n",
      "Epoch 343/500\n",
      "2507/2507 [==============================] - 0s 104us/sample - loss: 0.0013 - val_loss: 0.0110\n",
      "Epoch 344/500\n",
      "2507/2507 [==============================] - 0s 103us/sample - loss: 0.0013 - val_loss: 0.0110\n",
      "Epoch 345/500\n",
      "2507/2507 [==============================] - 0s 106us/sample - loss: 0.0013 - val_loss: 0.0111\n",
      "Epoch 346/500\n",
      "2507/2507 [==============================] - 0s 107us/sample - loss: 0.0013 - val_loss: 0.0110\n",
      "Epoch 347/500\n",
      "2507/2507 [==============================] - 0s 133us/sample - loss: 0.0013 - val_loss: 0.0108\n",
      "Epoch 348/500\n",
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0013 - val_loss: 0.0110\n",
      "Epoch 349/500\n",
      "2507/2507 [==============================] - 0s 111us/sample - loss: 0.0013 - val_loss: 0.0111\n",
      "Epoch 350/500\n",
      "2507/2507 [==============================] - 0s 100us/sample - loss: 0.0013 - val_loss: 0.0109\n",
      "Epoch 351/500\n",
      "2507/2507 [==============================] - 0s 110us/sample - loss: 0.0013 - val_loss: 0.0110\n",
      "Epoch 352/500\n",
      "2507/2507 [==============================] - 0s 133us/sample - loss: 0.0013 - val_loss: 0.0110\n",
      "Epoch 353/500\n",
      "2507/2507 [==============================] - 0s 108us/sample - loss: 0.0013 - val_loss: 0.0109\n",
      "Epoch 354/500\n",
      "2507/2507 [==============================] - 0s 103us/sample - loss: 0.0013 - val_loss: 0.0109\n",
      "Epoch 355/500\n",
      "2507/2507 [==============================] - 0s 98us/sample - loss: 0.0013 - val_loss: 0.0110\n",
      "Epoch 356/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0013 - val_loss: 0.0110\n",
      "Epoch 357/500\n",
      "2507/2507 [==============================] - 0s 111us/sample - loss: 0.0013 - val_loss: 0.0110\n",
      "Epoch 358/500\n",
      "2507/2507 [==============================] - 0s 109us/sample - loss: 0.0013 - val_loss: 0.0109\n",
      "Epoch 359/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0013 - val_loss: 0.0110\n",
      "Epoch 360/500\n",
      "2507/2507 [==============================] - 0s 107us/sample - loss: 0.0013 - val_loss: 0.0110\n",
      "Epoch 361/500\n",
      "2507/2507 [==============================] - 0s 107us/sample - loss: 0.0013 - val_loss: 0.0110\n",
      "Epoch 362/500\n",
      "2507/2507 [==============================] - 0s 112us/sample - loss: 0.0013 - val_loss: 0.0109\n",
      "Epoch 363/500\n",
      "2507/2507 [==============================] - 0s 107us/sample - loss: 0.0013 - val_loss: 0.0110\n",
      "Epoch 364/500\n",
      "2507/2507 [==============================] - 0s 102us/sample - loss: 0.0013 - val_loss: 0.0110\n",
      "Epoch 365/500\n",
      "2507/2507 [==============================] - 0s 103us/sample - loss: 0.0013 - val_loss: 0.0110\n",
      "Epoch 366/500\n",
      "2507/2507 [==============================] - 0s 100us/sample - loss: 0.0013 - val_loss: 0.0110\n",
      "Epoch 367/500\n",
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0012 - val_loss: 0.0110\n",
      "Epoch 368/500\n",
      "2507/2507 [==============================] - 0s 120us/sample - loss: 0.0012 - val_loss: 0.0110\n",
      "Epoch 369/500\n",
      "2507/2507 [==============================] - 0s 111us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 370/500\n",
      "2507/2507 [==============================] - 0s 102us/sample - loss: 0.0012 - val_loss: 0.0110\n",
      "Epoch 371/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 372/500\n",
      "2507/2507 [==============================] - 0s 108us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 373/500\n",
      "2507/2507 [==============================] - 0s 100us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 374/500\n",
      "2507/2507 [==============================] - 0s 109us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 375/500\n",
      "2507/2507 [==============================] - 0s 108us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 376/500\n",
      "2507/2507 [==============================] - 0s 117us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 377/500\n",
      "2507/2507 [==============================] - 0s 90us/sample - loss: 0.0012 - val_loss: 0.0110\n",
      "Epoch 378/500\n",
      "2507/2507 [==============================] - 0s 100us/sample - loss: 0.0012 - val_loss: 0.0110\n",
      "Epoch 379/500\n",
      "2507/2507 [==============================] - 0s 122us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 380/500\n",
      "2507/2507 [==============================] - 0s 119us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 381/500\n",
      "2507/2507 [==============================] - 0s 111us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 382/500\n",
      "2507/2507 [==============================] - 0s 102us/sample - loss: 0.0012 - val_loss: 0.0110\n",
      "Epoch 383/500\n",
      "2507/2507 [==============================] - 0s 107us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 384/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 385/500\n",
      "2507/2507 [==============================] - 0s 115us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 386/500\n",
      "2507/2507 [==============================] - 0s 112us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 387/500\n",
      "2507/2507 [==============================] - 0s 102us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 388/500\n",
      "2507/2507 [==============================] - 0s 103us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 389/500\n",
      "2507/2507 [==============================] - 0s 100us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 390/500\n",
      "2507/2507 [==============================] - 0s 105us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 391/500\n",
      "2507/2507 [==============================] - 0s 108us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 392/500\n",
      "2507/2507 [==============================] - 0s 112us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 393/500\n",
      "2507/2507 [==============================] - 0s 95us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 394/500\n",
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 395/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 396/500\n",
      "2507/2507 [==============================] - 0s 102us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 397/500\n",
      "2507/2507 [==============================] - 0s 125us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 398/500\n",
      "2507/2507 [==============================] - 0s 131us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 399/500\n",
      "2507/2507 [==============================] - 0s 102us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 400/500\n",
      "2507/2507 [==============================] - 0s 106us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 401/500\n",
      "2507/2507 [==============================] - 0s 103us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 402/500\n",
      "2507/2507 [==============================] - 0s 109us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 403/500\n",
      "2507/2507 [==============================] - 0s 102us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 404/500\n",
      "2507/2507 [==============================] - 0s 95us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 405/500\n",
      "2507/2507 [==============================] - 0s 96us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 406/500\n",
      "2507/2507 [==============================] - 0s 97us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 407/500\n",
      "2507/2507 [==============================] - 0s 113us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 408/500\n",
      "2507/2507 [==============================] - 0s 116us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 409/500\n",
      "2507/2507 [==============================] - 0s 102us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 410/500\n",
      "2507/2507 [==============================] - 0s 102us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 411/500\n",
      "2507/2507 [==============================] - 0s 123us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 412/500\n",
      "2507/2507 [==============================] - 0s 115us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 413/500\n",
      "2507/2507 [==============================] - 0s 140us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 414/500\n",
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 415/500\n",
      "2507/2507 [==============================] - 0s 95us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 416/500\n",
      "2507/2507 [==============================] - 0s 97us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 417/500\n",
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 418/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 419/500\n",
      "2507/2507 [==============================] - 0s 106us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 420/500\n",
      "2507/2507 [==============================] - 0s 110us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 421/500\n",
      "2507/2507 [==============================] - 0s 107us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 422/500\n",
      "2507/2507 [==============================] - 0s 97us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 423/500\n",
      "2507/2507 [==============================] - 0s 132us/sample - loss: 0.0012 - val_loss: 0.0109\n",
      "Epoch 424/500\n",
      "2507/2507 [==============================] - 0s 121us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 425/500\n",
      "2507/2507 [==============================] - 0s 117us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 426/500\n",
      "2507/2507 [==============================] - 0s 98us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 427/500\n",
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 428/500\n",
      "2507/2507 [==============================] - 0s 98us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 429/500\n",
      "2507/2507 [==============================] - 0s 112us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 430/500\n",
      "2507/2507 [==============================] - 0s 124us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 431/500\n",
      "2507/2507 [==============================] - 0s 102us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 432/500\n",
      "2507/2507 [==============================] - 0s 112us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 433/500\n",
      "2507/2507 [==============================] - 0s 127us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 434/500\n",
      "2507/2507 [==============================] - 0s 104us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 435/500\n",
      "2507/2507 [==============================] - 0s 124us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 436/500\n",
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 437/500\n",
      "2507/2507 [==============================] - 0s 118us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 438/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 439/500\n",
      "2507/2507 [==============================] - 0s 122us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 440/500\n",
      "2507/2507 [==============================] - 0s 110us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 441/500\n",
      "2507/2507 [==============================] - 0s 120us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 442/500\n",
      "2507/2507 [==============================] - 0s 109us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 443/500\n",
      "2507/2507 [==============================] - 0s 112us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 444/500\n",
      "2507/2507 [==============================] - 0s 98us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 445/500\n",
      "2507/2507 [==============================] - 0s 105us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 446/500\n",
      "2507/2507 [==============================] - 0s 113us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 447/500\n",
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 448/500\n",
      "2507/2507 [==============================] - 0s 109us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 449/500\n",
      "2507/2507 [==============================] - 0s 96us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 450/500\n",
      "2507/2507 [==============================] - 0s 109us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 451/500\n",
      "2507/2507 [==============================] - 0s 109us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 452/500\n",
      "2507/2507 [==============================] - 0s 119us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 453/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2507/2507 [==============================] - 0s 90us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 454/500\n",
      "2507/2507 [==============================] - 0s 125us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 455/500\n",
      "2507/2507 [==============================] - 0s 93us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 456/500\n",
      "2507/2507 [==============================] - 0s 95us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 457/500\n",
      "2507/2507 [==============================] - 0s 126us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 458/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 459/500\n",
      "2507/2507 [==============================] - 0s 90us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 460/500\n",
      "2507/2507 [==============================] - 0s 104us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 461/500\n",
      "2507/2507 [==============================] - 0s 117us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 462/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 463/500\n",
      "2507/2507 [==============================] - 0s 105us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 464/500\n",
      "2507/2507 [==============================] - 0s 129us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 465/500\n",
      "2507/2507 [==============================] - 0s 92us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 466/500\n",
      "2507/2507 [==============================] - 0s 94us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 467/500\n",
      "2507/2507 [==============================] - 0s 95us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 468/500\n",
      "2507/2507 [==============================] - 0s 104us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 469/500\n",
      "2507/2507 [==============================] - 0s 121us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 470/500\n",
      "2507/2507 [==============================] - 0s 100us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 471/500\n",
      "2507/2507 [==============================] - 0s 87us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 472/500\n",
      "2507/2507 [==============================] - 0s 84us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 473/500\n",
      "2507/2507 [==============================] - 0s 90us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 474/500\n",
      "2507/2507 [==============================] - 0s 110us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 475/500\n",
      "2507/2507 [==============================] - 0s 112us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 476/500\n",
      "2507/2507 [==============================] - 0s 96us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 477/500\n",
      "2507/2507 [==============================] - 0s 95us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 478/500\n",
      "2507/2507 [==============================] - 0s 95us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 479/500\n",
      "2507/2507 [==============================] - 0s 92us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 480/500\n",
      "2507/2507 [==============================] - 0s 103us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 481/500\n",
      "2507/2507 [==============================] - 0s 110us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 482/500\n",
      "2507/2507 [==============================] - 0s 100us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 483/500\n",
      "2507/2507 [==============================] - 0s 98us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 484/500\n",
      "2507/2507 [==============================] - 0s 123us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 485/500\n",
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 486/500\n",
      "2507/2507 [==============================] - 0s 117us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 487/500\n",
      "2507/2507 [==============================] - 0s 108us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 488/500\n",
      "2507/2507 [==============================] - 0s 99us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 489/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 490/500\n",
      "2507/2507 [==============================] - 0s 110us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 491/500\n",
      "2507/2507 [==============================] - 0s 102us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 492/500\n",
      "2507/2507 [==============================] - 0s 110us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 493/500\n",
      "2507/2507 [==============================] - 0s 120us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 494/500\n",
      "2507/2507 [==============================] - 0s 157us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 495/500\n",
      "2507/2507 [==============================] - 0s 117us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 496/500\n",
      "2507/2507 [==============================] - 0s 111us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 497/500\n",
      "2507/2507 [==============================] - 0s 107us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 498/500\n",
      "2507/2507 [==============================] - 0s 104us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 499/500\n",
      "2507/2507 [==============================] - 0s 110us/sample - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 500/500\n",
      "2507/2507 [==============================] - 0s 101us/sample - loss: 0.0011 - val_loss: 0.0109\n"
     ]
    }
   ],
   "source": [
    "model = getModel()\n",
    "\n",
    "xs = train.drop(columns=['energy'])\n",
    "ys = train.energy\n",
    "\n",
    "\n",
    "RedLR = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",factor=0.5,\n",
    "                                          patience=20,verbose=0)\n",
    "CheckP = keras.callbacks.ModelCheckpoint('./PES.hdf5',monitor=\"val_loss\",save_best_only=True)\n",
    "\n",
    "\n",
    "model.fit(xs,ys,batch_size=32,epochs=500,validation_split=0.15,\n",
    "          callbacks=[RedLR,CheckP]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getModel()\n",
    "model.load_weights('./PES.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E(coords):\n",
    "    return model.predict(coords) + minE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reported average error on the referenced paper is 5.0 kJ/mol (0.0019 Hartree). 0.0038 (0.0032) is about 2x this error, however it must be noted the paper used energies, gradients and hessians to interpolate using Taylor expansions, while here we're only using energies, so these are pretty nice results for such a cheap approach.\n",
    "\n",
    "Let's do some simple tests on this new surface. First, check topology. We want to see if the PES actually resembles some paths of interest, such as intrinsic reaction coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>energy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.012577</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>1.235442</td>\n",
       "      <td>0.003271</td>\n",
       "      <td>-0.002855</td>\n",
       "      <td>0.024893</td>\n",
       "      <td>0.751580</td>\n",
       "      <td>-0.008228</td>\n",
       "      <td>-0.786995</td>\n",
       "      <td>-1.091236</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.682590</td>\n",
       "      <td>-113.767379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.018907</td>\n",
       "      <td>0.005966</td>\n",
       "      <td>0.571072</td>\n",
       "      <td>0.004451</td>\n",
       "      <td>-0.006670</td>\n",
       "      <td>-0.636288</td>\n",
       "      <td>0.747959</td>\n",
       "      <td>-0.012130</td>\n",
       "      <td>-1.454406</td>\n",
       "      <td>-1.101017</td>\n",
       "      <td>-0.003128</td>\n",
       "      <td>-0.032748</td>\n",
       "      <td>-113.768423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.021828</td>\n",
       "      <td>0.005975</td>\n",
       "      <td>0.571503</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>-0.006618</td>\n",
       "      <td>-0.632455</td>\n",
       "      <td>0.740362</td>\n",
       "      <td>-0.012276</td>\n",
       "      <td>-1.456869</td>\n",
       "      <td>-1.115374</td>\n",
       "      <td>-0.003747</td>\n",
       "      <td>-0.082772</td>\n",
       "      <td>-113.771280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.024802</td>\n",
       "      <td>0.005986</td>\n",
       "      <td>0.571956</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>-0.006579</td>\n",
       "      <td>-0.628720</td>\n",
       "      <td>0.733078</td>\n",
       "      <td>-0.012350</td>\n",
       "      <td>-1.458419</td>\n",
       "      <td>-1.129613</td>\n",
       "      <td>-0.004325</td>\n",
       "      <td>-0.132897</td>\n",
       "      <td>-113.775410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.027789</td>\n",
       "      <td>0.005998</td>\n",
       "      <td>0.572433</td>\n",
       "      <td>-0.002037</td>\n",
       "      <td>-0.006540</td>\n",
       "      <td>-0.625033</td>\n",
       "      <td>0.725841</td>\n",
       "      <td>-0.012415</td>\n",
       "      <td>-1.459440</td>\n",
       "      <td>-1.142636</td>\n",
       "      <td>-0.004901</td>\n",
       "      <td>-0.183340</td>\n",
       "      <td>-113.780305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.012577  0.009804  1.235442  0.003271 -0.002855  0.024893  0.751580   \n",
       "1  0.018907  0.005966  0.571072  0.004451 -0.006670 -0.636288  0.747959   \n",
       "2  0.021828  0.005975  0.571503  0.002401 -0.006618 -0.632455  0.740362   \n",
       "3  0.024802  0.005986  0.571956  0.000243 -0.006579 -0.628720  0.733078   \n",
       "4  0.027789  0.005998  0.572433 -0.002037 -0.006540 -0.625033  0.725841   \n",
       "\n",
       "          7         8         9        10        11      energy  \n",
       "0 -0.008228 -0.786995 -1.091236  0.001279  0.682590 -113.767379  \n",
       "1 -0.012130 -1.454406 -1.101017 -0.003128 -0.032748 -113.768423  \n",
       "2 -0.012276 -1.456869 -1.115374 -0.003747 -0.082772 -113.771280  \n",
       "3 -0.012350 -1.458419 -1.129613 -0.004325 -0.132897 -113.775410  \n",
       "4 -0.012415 -1.459440 -1.142636 -0.004901 -0.183340 -113.780305  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IRC associated to TS3 (connects the two lowest energy configurations so it's important)\n",
    "def loadCoords(file='ts3.irc'):\n",
    "    #Empty df for holding training examples\n",
    "    df = pd.DataFrame({})\n",
    "    energyDF = []\n",
    "    with open(file,'r') as file:\n",
    "        line = 'True'\n",
    "        struct = pd.DataFrame({})\n",
    "        h_index = 1  #Index of the H atom, so as to distinguish both atoms\n",
    "        while line:\n",
    "            line = file.readline()\n",
    "            if not line: #If end of file, end loop\n",
    "                break \n",
    "\n",
    "            if line[:5] == 'Point':\n",
    "                energy = float(line.split('=')[1].strip()[:17])*1000\n",
    "                energyDF.append(energy)\n",
    "                #Restart struct df\n",
    "                struct = pd.DataFrame({})\n",
    "\n",
    "            elif line[0]!= '\\n' and line[0]!= '4': #If line not a name, then read structures\n",
    "                line = line.split(' ')\n",
    "                atom = line[0] #atom name\n",
    "\n",
    "                #Distinguish between the two H atoms: rename as H1 and H2\n",
    "                if atom == 'H':\n",
    "                    atom = atom + str(h_index)\n",
    "                    if h_index == 1: h_index += 1\n",
    "                    else: h_index = 1\n",
    "\n",
    "                #Read coordinates and append to struct df\n",
    "                coords = pd.Series(line[1:],name=atom).astype(float)\n",
    "                struct=struct.append(coords)\n",
    "\n",
    "                if struct.shape[0] == 4:  #If reading atoms is done, do computations\n",
    "                    struct = struct.loc[atoms] #Reorder atoms\n",
    "                    df = df.append(pd.Series(struct.values.ravel(),name=0))\n",
    "        df['energy'] = energyDF\n",
    "        return df.reset_index(drop=True)\n",
    "    \n",
    "#DF columns are: (x,y,z) coords for O,C,H1,H2, in that order (Ox, Oy, Oz, Cx, ...)\n",
    "irc = loadCoords()\n",
    "irc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Some operations before plotting\n",
    "tmp1 = irc.iloc[22:].sort_values(by='energy')\n",
    "tmp2 = irc.iloc[:22]\n",
    "IRC = pd.concat([tmp1,tmp2])\n",
    "\n",
    "#This is about the shape we should get from our NN on these coordinates\n",
    "plt.plot(IRC.energy.values);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate energies using selected method and basis set (UHF/cc-pVDZ)\n",
    "IRC['HF'] = IRC.drop(columns='energy').apply(HFenergy,axis=1)\n",
    "\n",
    "ircFeats = IRC.drop(columns=['HF','energy']).apply(getInput,axis=1)\n",
    "ircFeats['HF'] = IRC.HF\n",
    "\n",
    "columns = []\n",
    "#Same loops\n",
    "for i in range(len(atoms)):\n",
    "    for j in range(i+1,len(atoms)):  #Only atoms after ith (not counting it)\n",
    "        columns.append(atoms[i]+atoms[j])\n",
    "ircFeats.columns = columns + ['HF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Calculate NN-predicted energies\n",
    "predictedIRC = E(ircFeats.drop(columns='HF'))\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(predictedIRC,'k.',label='NN')\n",
    "ax.plot(ircFeats.HF.values,label='HF/cc-pVDZ')\n",
    "ax.legend()\n",
    "\n",
    "ax.set_title(r'IRC H$_2$CO $\\rightarrow$ HCOH')\n",
    "ax.set_ylabel('E / Hartree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, compute the gradients. TensorFlow allows to easily calculate gradients of tensors with respect to other tensors, upon which the former depend. For this purpose we need to initialize a GradientTape and within it perform the operations that lead to the output y = y(x), then compute the gradient. \n",
    "\n",
    "We compute this by using `X=getInput(xyz)` to get the inputs and then `E = model(X)`. Finally the gradient is computed. This is all implemented in the function `Grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.96391244e+01, -1.37495159e+02,  1.55958498e+01, -5.97237297e+01,\n",
       "        1.38057879e+02, -1.59760334e+01, -6.88860574e-02, -8.82527019e-01,\n",
       "        9.55460498e-02,  1.53491437e-01,  3.19806649e-01,  2.84637474e-01])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I better use TF for everything. \n",
    "#That is, calculate inputs as tensors and take gradients with respect to xyz\n",
    "def getInput(xyz):\n",
    "    \"\"\"xyz: vector containing xyz coords of each atom [Ox, Oy, Oz, Cx, ...]\n",
    "    pass xyz as tf.Variable(xyz)\"\"\"\n",
    "    xyzM = tf.reshape(xyz,(4,1,3))\n",
    "    xyzT = tf.transpose(xyzM,perm=[1,0,2])\n",
    "\n",
    "    DistMatr = tf.sqrt(tf.reduce_sum(tf.square(xyzM-xyzT),axis=2))\n",
    "\n",
    "    mask = np.zeros((4,4))\n",
    "    mask[np.triu_indices(4,1)] = 1\n",
    "\n",
    "    inps = 1./tf.boolean_mask(DistMatr,mask)\n",
    "    return tf.reshape(inps,(-1,6))\n",
    "\n",
    "def Grad(model,xyz):\n",
    "    \"\"\"Compute the gradients of E with respect to xyz coords\"\"\"\n",
    "    xyz = tf.Variable(xyz) #Convert xyz np.array to tf.Variable\n",
    "    with tf.GradientTape() as tape:  #Start recording operations\n",
    "        X = getInput(xyz)   #Convert to inputs\n",
    "        E = model(X)        #Calculate energies\n",
    "    return tape.gradient(E,xyz).numpy()   #Return gradient of E wrt xyz\n",
    "\n",
    "#Test\n",
    "xyz = np.random.random(12)\n",
    "Grad(model,xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trajectory:\n",
    "    def __init__(self,X0,V0,b=0,Nsteps=1000,timeDelta=1e-17,Xdim=12,saveMolden=False,gradient=Grad):\n",
    "        self.b = b\n",
    "        self.Nsteps = Nsteps\n",
    "        self.timeDelta = timeDelta\n",
    "        self.Xdim = Xdim #Dimensionality of the space\n",
    "        self.saveMolden = saveMolden\n",
    "        self.gradient = gradient\n",
    "        \n",
    "        #Simulation initial conditions\n",
    "        self.X0 = X0  #Initial coords (given in Ang)\n",
    "        self.V0 = V0  #Initial velocities (given in Ang/s)\n",
    "   \n",
    "        #Initialize data array  (3D tensor: quantities x timestep x dimension )\n",
    "        data=np.zeros((3,self.Nsteps,self.Xdim))\n",
    "        self.data = data\n",
    "    \n",
    "    def getAccel(self,t):\n",
    "        m = np.array([15.999,12.011,1.008,1.008])#Masses O,C,H1,H2\n",
    "        m = np.repeat(m,3)*1.660540199e-27   #kg\n",
    "        forces = -self.gradient(model,self.data[0,t])*431.75024  #kg*Ang/s**2\n",
    "        accel = forces/m   #Ang/s^2\n",
    "        return accel.reshape(1,-1)\n",
    "        \n",
    "    def run(self): #Using velocity verlet algorithm\n",
    "        dt=self.timeDelta\n",
    "        steps=self.Nsteps\n",
    "        data=self.data\n",
    "        \n",
    "        #data: x,v,a. Start trajectory: fist point\n",
    "        data[:2,0] = np.concatenate([self.X0, self.V0],axis=0) #Set initial position and velocities\n",
    "        data[2,0] = self.getAccel(0)            #Compute initial acceleration\n",
    "        \n",
    "        for i in range(steps-1):\n",
    "            data[0,i+1]=data[0,i]+data[1,i]*dt + 0.5*data[2,i]*dt*dt #update X\n",
    "            data[2,i+1]=self.getAccel(i+1)                        #update acceleration\n",
    "            tempV0_5=data[1,i]+0.5*data[2,i]*dt           #velocity at t+dt/2 (temporal)\n",
    "            data[1,i+1]=tempV0_5 + 0.5*data[2,i+1]*dt     #velocity at t+dt\n",
    "        self.data=data\n",
    "        print(\"Trajectory completed!\")\n",
    "        \n",
    "        if self.saveMolden:\n",
    "            self.saveTrajectMOLDEN()\n",
    "    \n",
    "    def getData(self):\n",
    "        return self.data\n",
    "    \n",
    "    def saveTrajectMOLDEN(self):\n",
    "        \"\"\"Pass as input a trayectory object\"\"\"\n",
    "        coords = self.data[0]\n",
    "        with open(self.saveMolden, \"w\") as f:\n",
    "            for i,struct in enumerate(coords):\n",
    "                print(\"4\\nPoint {}\\nO {:.8f} {:.8f} {:.8f}\\nC {:.8f} {:.8f} {:.8f}\\nH {:.8f} {:.8f} {:.8f}\\nH {:.8f} {:.8f} {:.8f}\".format(i,*struct),file=f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms are set up. Now only need to set initial conditions (position, velocity)\n",
    "We want to simulate the reaction H + HCO$^+$. We'll be crashing the two fragments and look for what comes out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  -1.1  0.   0.   0.   0.   0.7  0.6  0.  -3.7 -5.6  0. ]]\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  1.10251453e+14  1.66867064e+14 -0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#Define fragment one (HCO+) at the origin\n",
    "xOCH = np.zeros(9)\n",
    "xOCH[1] = -1.1  #O-C distance (along y axis)\n",
    "xOCH[6] = 0.7   #Hx\n",
    "xOCH[7] = 0.6   #Hy\n",
    "\n",
    "#Define the H fragment position \n",
    "xH = np.zeros(3) #\n",
    "xH[0] = -3.7   # Angstrom to the x axis\n",
    "xH[1] = -5.6\n",
    "X0 = np.concatenate([xOCH,xH]).reshape(1,-1)# + np.random.random(12)*0.04\n",
    "\n",
    "#Now define velocities (only reactive trajectories occur from colissions)\n",
    "unit = xH - xOCH[3:6]\n",
    "unit = unit/np.linalg.norm(unit)   #Unit vector pointing in the direction HCO to H\n",
    "\n",
    "v0 = 20000*1e10 #Relative velocity of the fragments (Ang/s)\n",
    "vH = v0*unit\n",
    "\n",
    "V0 = np.zeros(12).reshape(1,-1) \n",
    "V0[:,9:] = -vH.reshape(1,-1)  #Set velocity of the fragment\n",
    "\n",
    "print(X0)\n",
    "print(V0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9000 m/s is about 41 kJ/mol.\n",
    "\n",
    "Moyano et al report relative kinetic energies in the range 13.1 to 26.3 kJ/mol, which correspond to 5100 - 7223 m/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02837467,  0.02265754,  0.        ,  0.00200817, -0.03396399,\n",
       "        0.        , -0.00281522, -0.00377801,  0.        , -0.02756762,\n",
       "        0.01508447,  0.        ])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def HFgrad(model,row):\n",
    "    \"\"\"model is a dummy variable for compatibility\n",
    "    Real gradient at HF/cc-pVDZ.\n",
    "    This is expensive, use only for comparison\n",
    "    Documentation: http://psicode.org/psi4manual/1.3.2/opt.html\n",
    "    \"\"\"\n",
    "    psi4.set_memory('500 MB')\n",
    "\n",
    "    #charge = +1, spin multipicity = 2\n",
    "    h2o = psi4.geometry(\"\"\"\n",
    "    1 2\n",
    "    O {} {} {}\n",
    "    C {} {} {} \n",
    "    H {} {} {}\n",
    "    H {} {} {}\n",
    "    \"\"\".format(*row))\n",
    "    \n",
    "    psi4.set_options({'reference': 'uhf'})\n",
    "    \n",
    "    #Gradient calculated using UHF/cc-pVDZ\n",
    "    try: \n",
    "        #Gradient is returned in Hartree/bohr -> convert to Hartree/ang\n",
    "        return np.array(psi4.gradient('scf/cc-pvdz') ).flatten()  * 1.8897161 \n",
    "    except:  #In case there's a convergence error or alike\n",
    "        return 0.0\n",
    "    \n",
    "HFgrad(model,dataset.iloc[3,:-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04633983,  0.        , -0.13533161, -0.03922396,  0.        ,\n",
       "        0.13246559,  0.0351496 ,  0.        , -0.01164368,  0.05041419,\n",
       "        0.        ,  0.0145097 ])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Grad(model,dataset.iloc[3,:-1].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're pretty different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.02 s, sys: 44.4 ms, total: 1.07 s\n",
      "Wall time: 1.15 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.02837467,  0.02265754,  0.        ,  0.00200817, -0.03396399,\n",
       "        0.        , -0.00281522, -0.00377801,  0.        , -0.02756762,\n",
       "        0.01508447,  0.        ])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "HFgrad(model,dataset.iloc[3,:-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.4 ms, sys: 0 ns, total: 15.4 ms\n",
      "Wall time: 18.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.04633983,  0.        , -0.13533161, -0.03922396,  0.        ,\n",
       "        0.13246559,  0.0351496 ,  0.        , -0.01164368,  0.05041419,\n",
       "        0.        ,  0.0145097 ])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "Grad(model,dataset.iloc[3,:-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory completed!\n",
      "CPU times: user 30min 49s, sys: 30.5 s, total: 31min 20s\n",
      "Wall time: 33min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Let's run a traj with full HF gradients for comparison\n",
    "HFtraj = Trajectory(X0,V0,Nsteps=2000,timeDelta=1e-16,saveMolden='./HFtraj.txt',gradient=HFgrad)\n",
    "HFtraj.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory completed!\n",
      "CPU times: user 13.4 s, sys: 197 ms, total: 13.6 s\n",
      "Wall time: 13.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Now same traj with our NN PES\n",
    "NNtraj = Trajectory(X0,V0,Nsteps=2000,timeDelta=1e-16,saveMolden='./NNtraj_new.txt',gradient=Grad)\n",
    "NNtraj.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trajectories have been stored to the file `HFtraj.txt` and `NNtraj`. It can be viewed with some visualization program such as [MOLDEN](http://cheminf.cmbi.ru.nl/molden/molden.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step is to make the NN learn from these trajectories (as Moyano did). It will get better as more points of this trajectory are included in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psi4",
   "language": "python",
   "name": "psi4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
